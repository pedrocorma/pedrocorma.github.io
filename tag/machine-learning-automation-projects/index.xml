<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning Automation Projects | Pedro Cortés</title>
    <link>https://pedrocorma.github.io/tag/machine-learning-automation-projects/</link>
      <atom:link href="https://pedrocorma.github.io/tag/machine-learning-automation-projects/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine Learning Automation Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 18 May 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://pedrocorma.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Machine Learning Automation Projects</title>
      <link>https://pedrocorma.github.io/tag/machine-learning-automation-projects/</link>
    </image>
    
    <item>
      <title>Lead Scoring and Segmentation</title>
      <link>https://pedrocorma.github.io/project/2leadscoring/</link>
      <pubDate>Wed, 18 May 2022 00:00:00 +0000</pubDate>
      <guid>https://pedrocorma.github.io/project/2leadscoring/</guid>
      <description>&lt;h2 id=&#34;table-of-contents&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Table of contents&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#objectives&#34;&gt;Objectives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#understanding-the-problem&#34;&gt;Understanding the problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#project-design&#34;&gt;Project design&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-quality&#34;&gt;Data quality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#eda&#34;&gt;Exploratory data analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feature-transformation&#34;&gt;Feature transformation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lead-segmentation-model&#34;&gt;Lead segmentation model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lead-scoring-model&#34;&gt;Lead scoring model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-deployment&#34;&gt;Model deployment - Web app&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;br&gt;
&lt;ul class=&#34;cta-group&#34;&gt;
  
  &lt;li&gt;
    &lt;a href=&#34;https://p-03-notebooks03-systemapp-lead-scoringapp-lead-scoring-omfnil.streamlitapp.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34; class=&#34;btn btn-primary px-3 py-3&#34;&gt;Launch Lead Score Analyzer Web App!&lt;/a&gt;
  &lt;/li&gt;
  
  
&lt;/ul&gt;

&lt;div style=&#34;text-align: justify; font-size:0.9rem&#34;;&gt;
&lt;p&gt;
&lt;h2 id=&#34;introduction&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Introduction&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;The client is an online education company which sells an online course to industry professionals.&lt;/p&gt;
&lt;p&gt;The company markets their course on different websites and search engines. Once professionals who are interested in the course land on the website, they might browse the course or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals.&lt;/p&gt;
&lt;p&gt;Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not, with the inefficiency of this process impacting company&amp;rsquo;s benefits.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/br&gt;
&lt;em&gt;Notes:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This article presents a technical explanation of the development process followed in the project.&lt;/li&gt;
&lt;li&gt;Source code can be found &lt;a href=&#34;https://github.com/pedrocorma/lead-scoring-and-segmentation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Feel free to test the developed web application &lt;a href=&#34;https://p-03-notebooks03-systemapp-lead-scoringapp-lead-scoring-omfnil.streamlitapp.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;objectives&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Objectives&lt;/h2&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Analysing historical leads information to propose potential actions that increase overall company turnover.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Creating advanced analytical assets such as a
&lt;text id=&#34;tex_pred1&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;predictive lead scoring&lt;/text&gt; and &lt;text id=&#34;tex_customer&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;customer segmentation&lt;/text&gt;  algorithms that helps sales team to identify both potential customers who are most likely to convert into paying customers and leads who are not economically profitable to manage.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;understanding-the-problem&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Understanding the problem&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;When most companies start implementing inbound marketing, they&amp;rsquo;re primarily worried about getting enough new leads. However, once the marketing team defines and implements the right strategies and the systems the company has in place start working properly, getting enough leads is usually no longer a problem.&lt;/p&gt;
















&lt;figure  id=&#34;figure-inbound_marketing_stages&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Inbound marketing stages.&#34; srcset=&#34;
               /project/2leadscoring/inbound_marketing_stages_hua5332f1419fe79745a29948e7d71dc93_50789_6e8df19af7d183ea810e37a346c0e434.webp 400w,
               /project/2leadscoring/inbound_marketing_stages_hua5332f1419fe79745a29948e7d71dc93_50789_14a0e8a384c55ad01cca42dea6208fd2.webp 760w,
               /project/2leadscoring/inbound_marketing_stages_hua5332f1419fe79745a29948e7d71dc93_50789_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/2leadscoring/inbound_marketing_stages_hua5332f1419fe79745a29948e7d71dc93_50789_6e8df19af7d183ea810e37a346c0e434.webp&#34;
               width=&#34;714&#34;
               height=&#34;260&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Inbound marketing stages.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The challenge now is that in many cases the number of leads captured exceeds the capacity of the commercial channels, which generates problems such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Conflicts between marketing and sales departments: When there are so many leads coming in but not many sales being closed, these two departments may turn on one another. The marketing department does not understand how they are providing a large number of leads that Sales can not close. And the sales department believes that quality is more important than quantity, and they are not getting any good leads.&lt;/li&gt;
&lt;li&gt;Saturation of the commercial channel, given that each salesperson can manage a limited number of leads per day.&lt;/li&gt;
&lt;li&gt;Achieving fewer results than potentially possible, since there are a large number of leads that are not really interested in the company&amp;rsquo;s product and contact them takes time away from salespeople and prevents them from closing sales.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For all the above reasons, it becomes necessary to prioritize in order to reach out to the “best” leads quickly, while saving the “less likely” leads for last or leaving these “less likely” leads to be handled by more automated channels such as email.&lt;/p&gt;
&lt;p&gt;Nevertheless, businesses have struggled with prioritizing lead follow-up for decades. In many cases, salespeople are left to their own devices, using their best judgment to decide who gets contacted first. Marketers and salespeople use data such as demographic info (age, marital status, industry, role, &amp;hellip;), to rank potential customers as to how likely they are to buy. The problem with this approach is that it has a certain component of subjectivity. Salespeople are forced to rely on “gut feelings” and factor in their own historical experience to make this decision. Neither of these proves to be consistently accurate causing quality leads to slip through the cracks as they chase prospects unlikely to buy.&lt;/p&gt;
&lt;p&gt;On the other hand, to implement an effective inbound marketing strategy is vital to have knowledge about the type of leads interested in the company&amp;rsquo;s product to drive dynamic content and personalization tactics for timelier, relevant and more effective marketing communications. To this end, appropriate customer segmentation can help to cluster leads into groups sharing the same properties or behavioral characteristics instead of a ‘one-size-fits-all’ approach, which may prove to be in determine new market opportunities and improve brand strategy, marketing efficiency and customer retention between other benefits.&lt;/p&gt;
&lt;h2 id=&#34;project-design&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Project design&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;methodology&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Methodology&lt;/h3&gt;
&lt;p&gt;Traditionally there have been two main methodologies for advanced data modelling: CRISP-DM and SEMMA. Both methodologies structure the data mining project in phases that are interrelated, converting the process into iterative and interactive.&lt;/p&gt;
















&lt;figure  id=&#34;figure-crispdm_semma&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Traditional advanced data modelling methodologies: CRISP-DM (left) and SEMMA (right).&#34; srcset=&#34;
               /project/2leadscoring/crispdm_semma_huf3f4e44f28e1309b76f5d93c3dd1138b_411506_b32f6bedc07e8292ded13386302e288a.webp 400w,
               /project/2leadscoring/crispdm_semma_huf3f4e44f28e1309b76f5d93c3dd1138b_411506_d729433dd56148669153d79d4dc016c7.webp 760w,
               /project/2leadscoring/crispdm_semma_huf3f4e44f28e1309b76f5d93c3dd1138b_411506_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/2leadscoring/crispdm_semma_huf3f4e44f28e1309b76f5d93c3dd1138b_411506_b32f6bedc07e8292ded13386302e288a.webp&#34;
               width=&#34;760&#34;
               height=&#34;332&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Traditional advanced data modelling methodologies: CRISP-DM (left) and SEMMA (right).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;This project has been designed using a methodology halfway between the two presented above, as shown in the figure below.&lt;/p&gt;
















&lt;figure  id=&#34;figure-methodology&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Project methodology.&#34; srcset=&#34;
               /project/2leadscoring/methodology_hub1fcd17b8c9ee92ec5babce37572c8a5_161025_ca20d8e831509b355dad5fc7daf51fad.webp 400w,
               /project/2leadscoring/methodology_hub1fcd17b8c9ee92ec5babce37572c8a5_161025_e68eebc61670774f794b4ea06ddf7639.webp 760w,
               /project/2leadscoring/methodology_hub1fcd17b8c9ee92ec5babce37572c8a5_161025_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/2leadscoring/methodology_hub1fcd17b8c9ee92ec5babce37572c8a5_161025_ca20d8e831509b355dad5fc7daf51fad.webp&#34;
               width=&#34;760&#34;
               height=&#34;238&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Project methodology.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h3 id=&#34;levers&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Levers&lt;/h3&gt;
There is usually a limitless number of things a company can consider trying in order to improve their business.  However, the options should be narrowed down to include only the levers most relevant to the company and their situation and goals, which in this case are:
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;text id=&#34;leads_lever&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Leads:&lt;/text&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Obtain knowledge of the leads captured by the company to optimise future commercial campaigns.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;text id=&#34;cr_lever&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Conversion rate:&lt;/text&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the rate at which leads become customers is increased, the company&amp;rsquo;s profits will also increase.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;text id=&#34;channles_lever&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Comercial channels optimisation:&lt;/text&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The client uses different sales channels to communicate with leads (phone calls, sms, emails, web chat and a subcontracted lead management company). By knowing the propensity to purchase of each lead, the company will be able to prioritise the management of the most promising leads through the most appropriate commercial channel for each of them.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;kpis&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;KPIs&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;text id=&#34;conversion_rate&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Lead-to-customer conversion rate (CR)&lt;/text&gt; defined as:&lt;/li&gt;
&lt;/ol&gt;

$$
CR\,(\%) = \frac{N_{\text{customers}}}{N_{\text{leads}}} \cdot 100
$$

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$N_{\text{customers}}:$ number leads converted into paying customer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$N_{\text{leads}}:$ total number of leads.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;text id=&#34;sales_workload&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Sales team workload:&lt;/text&gt; number of potential customers to be managed by the sales team.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;text id=&#34;losts&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Lost investment in not converted lead management:&lt;/text&gt; cost of commercial actions carried out on potential customers who do not end up buying the company&amp;rsquo;s product.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;text id=&#34;sales_profit&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Sales profit (SP):&lt;/text&gt; total profit from the sale of the online courses, defined as:

$$
SP\,(\$) = N_{\text{customers}}\cdot (Price_{\text{product}}-Cost_{\text{lead to customer}})-Costs_{\text{leads not converted}}
$$
&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$N_{\text{customers}}:$ number leads converted into paying customer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$Price_{\text{product}}:$ price of the online course for industry professional.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$Cost_{\text{lead to customer}}:$ cost per lead arising from commercial and marketing actions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$Costs_{\text{leads not converted}}:$ lost investment in not converted lead management.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;entities-and-data&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Entities and data&lt;/h3&gt;
The entities relevant to the achievement of the project&#39;s objectives and for which data are available are:
&lt;ol&gt;
&lt;li&gt;&lt;text id=&#34;leads&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Leads:&lt;/text&gt;
&lt;ul&gt;
&lt;li&gt;The leads data available are those provided by the client in a &lt;em&gt;.csv&lt;/em&gt; file which contains information on 37 different features for 9240 different leads. Further information about the available features and their description are also provided in a &lt;em&gt;.pdf&lt;/em&gt; document.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
















&lt;figure  id=&#34;figure-raw_data&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Leads dataframe (raw data).&#34; srcset=&#34;
               /project/2leadscoring/raw_data_hu86041b5adf93ac8acda0fcfa05df4f43_39663_1ed0b8c2fee692550b31854cb8d88410.webp 400w,
               /project/2leadscoring/raw_data_hu86041b5adf93ac8acda0fcfa05df4f43_39663_67a1f8df599e4fbe138b6fb26fb07363.webp 760w,
               /project/2leadscoring/raw_data_hu86041b5adf93ac8acda0fcfa05df4f43_39663_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/2leadscoring/raw_data_hu86041b5adf93ac8acda0fcfa05df4f43_39663_1ed0b8c2fee692550b31854cb8d88410.webp&#34;
               width=&#34;760&#34;
               height=&#34;343&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Leads dataframe (raw data).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;text id=&#34;product&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Product:&lt;/text&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Description: Online course for industry professional.&lt;/li&gt;
&lt;li&gt;Price: 49.99 $.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;text id=&#34;comercial_channels&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Comercial channels optimisation:&lt;/text&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Phone calls, Sms, Email, Olark Chat, Subcontracted lead management company, Ad campaigns.&lt;/li&gt;
&lt;li&gt;Lead management average cost: 3.25 $ per lead.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;data-quality&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Data quality&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;In this stage of the project, general data quality correction processes have been applied, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Feature renaming&lt;/li&gt;
&lt;li&gt;Feature type correction&lt;/li&gt;
&lt;li&gt;Elimination of features with unique values&lt;/li&gt;
&lt;li&gt;Nulls imputation&lt;/li&gt;
&lt;li&gt;Outliers management&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The entire process can be consulted in detail &lt;a href=&#34;https://github.com/pedrocorma/lead-scoring-and-segmentation/blob/main/03_Notebooks/02_Development/02_Data%20Quality.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;eda&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Exploratory data analysis&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;The aim of this stage of the project is to discover trends, patterns, and to check assumptions with the help of statistical summary and graphical representations. Complete analysis can be found &lt;a href=&#34;https://github.com/pedrocorma/lead-scoring-and-segmentation/blob/main/03_Notebooks/02_Development/03_EDA.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In order to guide the process, a series of seed questions were posed to serve as a basis for developing and deepening the analysis of the different features.&lt;/p&gt;
&lt;h3 id=&#34;seed_questions&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Seed questions&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Regarding Leads:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;text id=&#34;q1&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Q1:&lt;/text&gt; What are the main demographic profiles in the company&amp;rsquo;s lead database?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Regarding Conversion Rate:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;text id=&#34;q2&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Q2:&lt;/text&gt; What is the lead-to-customer conversion rate the company is currently achieving?&lt;/li&gt;
&lt;li&gt;&lt;text id=&#34;q3&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Q3:&lt;/text&gt; What factors affect lead-to-customer conversion the most?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Regarding commercial and marketing channels:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;text id=&#34;q4&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Q4:&lt;/text&gt; How are the company&amp;rsquo;s commercial/marketing channels performing?&lt;/li&gt;
&lt;li&gt;&lt;text id=&#34;q5&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Q5:&lt;/text&gt; From what sources is the company attracting potential customers? Which are the most promising ones?&lt;/li&gt;
&lt;li&gt;&lt;text id=&#34;q6&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Q6:&lt;/text&gt; Which demographic profile should be the main focus of marketing actions?&lt;/li&gt;
&lt;li&gt;&lt;text id=&#34;q7&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Q7:&lt;/text&gt; What percentage of leads are open to receiving communications by email or phone calls?&lt;/li&gt;
&lt;li&gt;&lt;text id=&#34;q8&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Q8:&lt;/text&gt; How did the company&amp;rsquo;s advertising campaigns perform?&lt;/li&gt;
&lt;li&gt;&lt;text id=&#34;q9&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Q9:&lt;/text&gt; How is the current lead magnet performing?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example of the analysis of some of the categorical features present in the dataset:
















&lt;figure  id=&#34;figure-eda_cat&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Exploratory data analysis: lead source and current ocupation.&#34; srcset=&#34;
               /project/2leadscoring/eda_cat_hu1260394cf938494760f66ed372224765_85745_05b1cf12ce15c45612c510297ef167db.webp 400w,
               /project/2leadscoring/eda_cat_hu1260394cf938494760f66ed372224765_85745_4483b24006c0a5671eb8850b7900d5ce.webp 760w,
               /project/2leadscoring/eda_cat_hu1260394cf938494760f66ed372224765_85745_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/2leadscoring/eda_cat_hu1260394cf938494760f66ed372224765_85745_05b1cf12ce15c45612c510297ef167db.webp&#34;
               width=&#34;760&#34;
               height=&#34;592&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Exploratory data analysis: lead source and current ocupation.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;Example of one of the analyses carried out on numerical features present in the dataset:
















&lt;figure  id=&#34;figure-eda_num&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Exploratory data analysis: numerical features.&#34; srcset=&#34;
               /project/2leadscoring/eda_num_hub81884994ab0256dda7f18b4581ee1ab_119191_0cd37d8c0018ea5a609c2f6e33a254d6.webp 400w,
               /project/2leadscoring/eda_num_hub81884994ab0256dda7f18b4581ee1ab_119191_fb84d2621cf779b2adaee03bda0249dd.webp 760w,
               /project/2leadscoring/eda_num_hub81884994ab0256dda7f18b4581ee1ab_119191_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/2leadscoring/eda_num_hub81884994ab0256dda7f18b4581ee1ab_119191_0cd37d8c0018ea5a609c2f6e33a254d6.webp&#34;
               width=&#34;676&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Exploratory data analysis: numerical features.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h3 id=&#34;eda_insights&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Insights&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Leads:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The vast majority (89,7%) of the interested customers that the company is currently attracting are unemployed.&lt;/li&gt;
&lt;li&gt;Only 7.7% of leads are working professionals.&lt;/li&gt;
&lt;li&gt;Activity and profile index features are missing for 45.4% of the leads.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Lead-to-customer conversion rate:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Current lead-to-customer conversion rate is 38.6%.&lt;/li&gt;
&lt;li&gt;Working professionals have high conversion rate (92.5%), especially those from management sector.&lt;/li&gt;
&lt;li&gt;Unemploeyed leads, although high in number, have low conversion rate (33.9%).&lt;/li&gt;
&lt;li&gt;Almost all leads coming from &amp;lsquo;Reference&amp;rsquo; (91.7% conv. rate) and &amp;lsquo;Welingak Website&amp;rsquo; (98.1% conv. rate) sources end up buying the product. However, only 2.8% of leads come from these sources.&lt;/li&gt;
&lt;li&gt;Leads who hear about the company through recommendations (World of mouth, Student of someschool) have higher conversion rates.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Commercial and marketing channels:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Converted leads spent a median of 10 minutes more time viewing the website than those who did not convert, regardless to the amount of visits and number of pages viewed.&lt;/li&gt;
&lt;li&gt;Only 0.24% of the leads indicated on the form that they have seen advertisements from the company. Around 92% of leads do not like to be called or receive emails about the course.&lt;/li&gt;
&lt;li&gt;Employees are not sure about the quality of 63% of leads, and only have time/information to fill in the lead profile field for 26.1% of all leads.&lt;/li&gt;
&lt;li&gt;Email marketing campaigns have untapped potential, as the last notable activity/last activity of 30%-37% of total number of leads was opening an email but only about 37% of them were converted after it. Only 14.8% of leads who want to be contacted by email end up converting into paying customers.&lt;/li&gt;
&lt;li&gt;Sms campaigns achieved conversion rates of 60%-70% and reached a significant number of leads.&lt;/li&gt;
&lt;li&gt;There are leads tagged as &amp;lsquo;Ringing&amp;rsquo; who selected &amp;lsquo;do not want to receive phone calls&amp;rsquo; on the form.&lt;/li&gt;
&lt;li&gt;Most potential customers were not interested in getting a free copy of the lead magnet. Leads interested in downloading the lead magnet are mostly unemployed and download it mainly from the landing page.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;eda_actions_leads&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Actions to improve company&#39;s customer knowledge&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Improve quality of the survey or form questions to receive more user inputs and reduce NaN/default (&amp;lsquo;Select&amp;rsquo;) values.&lt;/li&gt;
&lt;li&gt;Improve algorithm for the activity and profile score/index to produce complete and more accurate results.&lt;/li&gt;
&lt;li&gt;Colect time stamp visiting the website for seasonality analysis and implement cookies to identity and track users as they navigate different pages on the website.&lt;/li&gt;
&lt;li&gt;Implement a new &lt;text id=&#34;text_lead_segmentation&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;lead segmentation algorithm&lt;/text&gt; that identifies the company&amp;rsquo;s different leads profiles and makes it possible to identify which group best fit for each new lead, in order to be able to carry out more personalised commercial actions.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;eda_actions_cr&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Actions to improve lead-to-customer conversion rate&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Implement a &lt;text id=&#34;text_lead_segmentation&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;predictive lead scoring algorithm&lt;/text&gt; that identifies people who are most likely to convert into paying customers and relieves the sales team of the workload of manually filling in features such as lead_quality, lead_profile or tags so they will be able to spend more time on contacting the most promising customers.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;eda_actions_channels&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Actions to improve commercial and marketing channels performance&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Improve the content strategy of the website, lead magnet and emails to attract traffic and increase the time people spend on the website by creating tailored content mainly for working professionals in the Indian management sector.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a referral program to encourage existing customers to recommend the course to their friends, family, and colleagues.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Invest more resources into acquiring leads from &amp;lsquo;Welingak website&amp;rsquo;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Increase investments in SMS campaigns as they are performing well.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Check whether the default value for advertisement features is set to &amp;lsquo;No&amp;rsquo; in the web form, which could explain the high percentage of &amp;lsquo;No&amp;rsquo; for all of them. If this is not the case, then the advertising investment strategy should be completely revised as it is not generating almost any lead (0,24%).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Check that the sales team is only contacting people who have given their consent to do so.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;feature-transformation&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Feature transformation&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;At this stage of the project, different variable transformation techniques will be applied to adapt them to the requirements of the algorithms that will be used during the modelling phase.&lt;/p&gt;
&lt;p&gt;As discussed during the exploratory data analysis stage, two different models will be developed:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A lead segmentation model that helps sales and marketing teams to identify the company&amp;rsquo;s different leads profiles.&lt;/li&gt;
&lt;li&gt;A predictive lead scoring model that identifies people who are most likely to convert into paying customers.
In both cases, categorical features have to be transformed into numerical features. Given that the categorical features in the dataset are of the nominal type, one hot encoding technique will be used for this purpose.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Unsupervised modelling techniques based on Kmeans algorithm will be used for lead segmentation model. Kmeans is very sensitive to the different scales of the features as it is a distance-based algorithm, therefore rescaling techniques have to be applied to ensure that all features are on the same scale. Since it has been decided to apply one hot encoding to categorical features, the rescaling technique that makes the most sense to apply in this case is min-max scaling which will allow transforming feature values to a scale between 0 and 1.&lt;/p&gt;
&lt;p&gt;On the other hand, it has to be decided whether feature discretisation/binarisation processes are to be applied. Given that for the project to be developed the objective of prediction is more important than interpretation one, and also taking into account that one of the models to be developed is based on a segmentation algorithm, neither discretisation nor binarisation processes will be applied.&lt;/p&gt;
&lt;p&gt;Finally, note that it is not necessary to apply class balancing processes as the presence of both classes in the dataset (converted=1, converted=0) is sufficiently significant.&lt;/p&gt;
&lt;h2 id=&#34;lead-segmentation-model&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Lead segmentation model&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Customer segmentation based on unsupervised modelling will be developed by applying the Kmeans algorithm, in order to find new insights which help to improve company&amp;rsquo;s sales and marketing campaigns and so increase lead-to-customer conversion rate.&lt;/p&gt;
&lt;h3 id=&#34;number_segments&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Selecting the number of segments&lt;/h3&gt;
&lt;p&gt;Different methods have been used to identify the optimal number of clusters to be considered in the application of the Kmeans algorithm.&lt;/p&gt;
















&lt;figure  id=&#34;figure-segmentation_k&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Methods to identify the optimal numer of clusters of Kmeans algorithm.&#34; srcset=&#34;
               /project/2leadscoring/segmentation_k_hubc7080fd22f31aef6f2ee68ab71dded6_51193_ff3eb970ee2fe3d37ce3232fac2f3e48.webp 400w,
               /project/2leadscoring/segmentation_k_hubc7080fd22f31aef6f2ee68ab71dded6_51193_9fbbbe7eb59d97fe16b53777e3bced9b.webp 760w,
               /project/2leadscoring/segmentation_k_hubc7080fd22f31aef6f2ee68ab71dded6_51193_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/2leadscoring/segmentation_k_hubc7080fd22f31aef6f2ee68ab71dded6_51193_ff3eb970ee2fe3d37ce3232fac2f3e48.webp&#34;
               width=&#34;708&#34;
               height=&#34;564&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Methods to identify the optimal numer of clusters of Kmeans algorithm.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Analysing the above graphs, it can be seen that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Elbow method: It does not provide useful information in this case as it shows a linear decrease in errors.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Silhouette method: Better results with increasing number of clusters.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Calinski-Harabasz index: 2 or 3 clusters provide the best solutions. Results with 6 or 7 clusters are better than with 4 or 5.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Davies-Bouldin index: Better results with increasing number of clusters.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After an iterative process, it is found that the number of clusters that provides the most business meaningful segmentation is 6.&lt;/p&gt;
&lt;h3 id=&#34;segment_profiling&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Segment profiling&lt;/h3&gt;
&lt;p&gt;Once the optimal number of clusters has been selected, and the segmentation features to be used have been chosen, the model is trained and executed, assigning each lead to one of the six existing clusters.&lt;/p&gt;
&lt;p&gt;Average values have been calculated for each of the features introduced in the model, in order to try to identify which are the most differentiating characteristics of each segment from a business point of view.&lt;/p&gt;
















&lt;figure  id=&#34;figure-segmentation_result&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Unsupervised modelling: clustering results.&#34; srcset=&#34;
               /project/2leadscoring/segmentation_result_hu286b2c82874cbd00841e46e58634c52e_50626_6fd5eabc0053d3b432ab56aa17e4c153.webp 400w,
               /project/2leadscoring/segmentation_result_hu286b2c82874cbd00841e46e58634c52e_50626_4289aecee4a5b219f5bd0b47b0ea1a53.webp 760w,
               /project/2leadscoring/segmentation_result_hu286b2c82874cbd00841e46e58634c52e_50626_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/2leadscoring/segmentation_result_hu286b2c82874cbd00841e46e58634c52e_50626_6fd5eabc0053d3b432ab56aa17e4c153.webp&#34;
               width=&#34;760&#34;
               height=&#34;622&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Unsupervised modelling: clustering results.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h3 id=&#34;segment_profiling&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Segment descriptions&lt;/h3&gt;
&lt;p&gt;After analysing the above results, the descriptions of the most differential points of each of the segments identified by the algorithm are presented.&lt;/p&gt;
&lt;p&gt;&lt;text id=&#34;s0&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Segment 0:&lt;/text&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Origin: API.&lt;/li&gt;
&lt;li&gt;Last activity: Most leads have conversations via Olark chat.&lt;/li&gt;
&lt;li&gt;Segment with lower presence of working professionals.&lt;/li&gt;
&lt;li&gt;Time spent on the website far below average.&lt;/li&gt;
&lt;li&gt;Almost no leads in this segment buy the company&amp;rsquo;s product.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;text id=&#34;s1&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Segment 1:&lt;/text&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Origin: Landing Page.&lt;/li&gt;
&lt;li&gt;Last activity: Email Opened.&lt;/li&gt;
&lt;li&gt;Some presence of working professionals.&lt;/li&gt;
&lt;li&gt;Above-average time spent on the website.&lt;/li&gt;
&lt;li&gt;Slightly lower conversion rate than the company&amp;rsquo;s current average conversion rate.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;text id=&#34;s2&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Segment 2:&lt;/text&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Origin: Landing Page.&lt;/li&gt;
&lt;li&gt;Last activity: Most of them have received an SMS. Some of them have visited the website.&lt;/li&gt;
&lt;li&gt;Some presence of working professionals.&lt;/li&gt;
&lt;li&gt;Above-average time spent on the website.&lt;/li&gt;
&lt;li&gt;Slightly higher conversion rate than the company&amp;rsquo;s current average conversion rate.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;text id=&#34;s3&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Segment 3:&lt;/text&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Origin: API.&lt;/li&gt;
&lt;li&gt;Last activity: Email Opened.&lt;/li&gt;
&lt;li&gt;Some presence of working professionals.&lt;/li&gt;
&lt;li&gt;Below average time spent on website.&lt;/li&gt;
&lt;li&gt;Slightly lower conversion rate than the company&amp;rsquo;s current one. Similar to the conversion rate of segment 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;text id=&#34;s4&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Segment 4:&lt;/text&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Origin: Lead Add Form (main sources: References, Welingak website).&lt;/li&gt;
&lt;li&gt;Last activity: Email Opened, SMS sent or unknown.&lt;/li&gt;
&lt;li&gt;High presence of working professionals.&lt;/li&gt;
&lt;li&gt;Time spent on the website far below average.&lt;/li&gt;
&lt;li&gt;9 out of 10 leads in this segment end up buying the company&amp;rsquo;s product.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;text id=&#34;s5&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Segment 5:&lt;/text&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Origin: API.&lt;/li&gt;
&lt;li&gt;Last activity: SMS sent.&lt;/li&gt;
&lt;li&gt;Notable presence of working professionals.&lt;/li&gt;
&lt;li&gt;Above-average time spent on the website.&lt;/li&gt;
&lt;li&gt;Conversion rate significantly higher than the company&amp;rsquo;s current average conversion rate.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;segment_insights&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Segmentation insights&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The company&amp;rsquo;s most valuable leads are those that come from referrals or from Welingak website, and even more so if they are working professionals. As proposed in the exploratory data analysis section, the company should seriously consider creating a referral programme to encourage existing customers to recommend the course to their close circle.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SMS campaigns are performing quite well. However, these campaigns should focus on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Working professionals comming from API or landing page who spend above-average time on the website.&lt;/li&gt;
&lt;li&gt;Leads comming from References or Welingak website regardless of their occupation and time spend on the website.&lt;/li&gt;
&lt;li&gt;Avoid sending sms to leads who come from API and have spent a short time on the site.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Olark chat is not performing well. The company should consider withdrawing investment in this service and for leads coming from API replace it with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Email marketing campaigns in case of working professionals who spent a short time on the website or in case of unemployed leads.&lt;/li&gt;
&lt;li&gt;SMS campaigns in case of working professionals who spend above-average time on the website, as discussed in the previous point.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;lead-scoring-model&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Lead scoring model&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;feature_selection&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Feature selection&lt;/h3&gt;
At this stage of the project most useful features to a model in order to predict the target variable will be selected by comparing the results of different selection methods, to both improve the performance of the model and reduce the computational cost of modelling.
















&lt;figure  id=&#34;figure-mutual_information&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Feature importance: mutual information method.&#34; srcset=&#34;
               /project/2leadscoring/mutual_information_hu94e8d50e8a111dd8cbbd6d8457447c47_84200_86dfa70edbe768b445f113e0c9db3f0c.webp 400w,
               /project/2leadscoring/mutual_information_hu94e8d50e8a111dd8cbbd6d8457447c47_84200_c44000e684473f52188bf6aa8f7f4794.webp 760w,
               /project/2leadscoring/mutual_information_hu94e8d50e8a111dd8cbbd6d8457447c47_84200_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/2leadscoring/mutual_information_hu94e8d50e8a111dd8cbbd6d8457447c47_84200_86dfa70edbe768b445f113e0c9db3f0c.webp&#34;
               width=&#34;760&#34;
               height=&#34;590&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Feature importance: mutual information method.
    &lt;/figcaption&gt;&lt;/figure&gt;
















&lt;figure  id=&#34;figure-recursive_feature_elimination&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Feature importance: recursive feature elimination method.&#34; srcset=&#34;
               /project/2leadscoring/recursive_feature_elimination_hub27d0ffcf1a44496083239793e007375_90557_21f51efe47cec717c83612737206825e.webp 400w,
               /project/2leadscoring/recursive_feature_elimination_hub27d0ffcf1a44496083239793e007375_90557_1ecd47d7cfbe3eab6cd3d1f422057ef7.webp 760w,
               /project/2leadscoring/recursive_feature_elimination_hub27d0ffcf1a44496083239793e007375_90557_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/2leadscoring/recursive_feature_elimination_hub27d0ffcf1a44496083239793e007375_90557_21f51efe47cec717c83612737206825e.webp&#34;
               width=&#34;760&#34;
               height=&#34;589&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Feature importance: recursive feature elimination method.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The results provided by the feature selection methods analysed are similar in terms of the number and features selected. However, features selected by mutual information method are taken as a basis since features such as &amp;lsquo;source_Reference&amp;rsquo; or &amp;lsquo;source_Olark Chat&amp;rsquo; are being taken into account, which could be predictives as discussed in the exploratory data analysis and unsupervised customer segmentation stages.&lt;/p&gt;
&lt;p&gt;After that, the set of pre-selected features is tested for significant correlations, as the algorithm to be used has not yet been decided at this stage of the project. The existence of strong correlations, while not a drawback for tree-based algorithms, may reduce the performance of other algorithms such as logistic regressions, etc.&lt;/p&gt;
&lt;p&gt;A compromise is therefore sought between the predictive ability of each feature and the number and importance of its correlations with other pre-selected features.&lt;/p&gt;
&lt;p&gt;Finally, after eliminating 9 of the features, the final set of features to be introduced into the predictive model is obtained.&lt;/p&gt;
&lt;h3 id=&#34;model_selection&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Model selection&lt;/h3&gt;
Different combinations of algorithms/hyperparameters have been tested to find those with the best performance.
&lt;p&gt;Models obtaining the best ROC AUC scores are those based on Gradient Boosted Machines (XGBoost, LightGBM). However, there does not seem to be much difference with respect to the scores obtained in the best parameterisations of logistic regression algorithms.&lt;/p&gt;
&lt;p&gt;On the other hand, scores obtained in the tested algorithms/parametrisations remains stable during cross-validation process, which is a good indicator of the stability of the model predictions.&lt;/p&gt;
&lt;p&gt;The best parameterisation of each type of algorithm (XGBoost, LightGBM, Logistic Regression) will be selected in order to analyse in detail whether there are significant differences in their performance that could lead to the selection of the final algorithm architecture to be used.&lt;/p&gt;
















&lt;figure  id=&#34;figure-roc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Comparaison of ROC AUC curves. AUC scores: 0.924 (LightGBM), 0.926 (XGBoost), 0.91 (Logistic regression).&#34; srcset=&#34;
               /project/2leadscoring/ROC_hubd8378d47da96bde46b5a085a7d658c9_42730_309641060910d08ba1c8852cae0c258a.webp 400w,
               /project/2leadscoring/ROC_hubd8378d47da96bde46b5a085a7d658c9_42730_3a3a5846424a426df28b1e6089a18a01.webp 760w,
               /project/2leadscoring/ROC_hubd8378d47da96bde46b5a085a7d658c9_42730_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/2leadscoring/ROC_hubd8378d47da96bde46b5a085a7d658c9_42730_309641060910d08ba1c8852cae0c258a.webp&#34;
               width=&#34;760&#34;
               height=&#34;394&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Comparaison of ROC AUC curves. AUC scores: 0.924 (LightGBM), 0.926 (XGBoost), 0.91 (Logistic regression).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;As can be seen from the graph, there are no major differences in the performance of the three models compared.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ROC curves of LightGBM and XGBoost models are almost identical, as is their predictive ability.&lt;/li&gt;
&lt;li&gt;Model based on the logistic regression algorithm has a slightly worse ROC curve than the two tree-based models.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It was decided to implement the logistic regression algorithm for the project due to the following reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Its predictive ability is very similar to that obtained with the LightGBM and XGBoost models.&lt;/li&gt;
&lt;li&gt;It is a much more interpretable model than LightGBM or XGBoost algorithms.&lt;/li&gt;
&lt;li&gt;It is a simpler model, therefore easier to maintain and quicker to train, re-train and execute.&lt;/li&gt;
&lt;li&gt;It can be easily migrated to the platforms and software currently used by the company due to its mathematical simplicity.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;threshold&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Selecting optimal discrimination threshold that maximises ROI&lt;/h3&gt;
This section analyses what is the optimal discrimination threshold value that maximises the company&#39;s return on investment. In this way, in addition to the scoring, the model developed will indicate for each customer whether it should be subject to commercial actions or whether it is not worth investing resources in managing it.
&lt;p&gt;As reported by the company, its revenue and cost scenario is as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Product selling price (online course for industry professional): 49.99 $&lt;/li&gt;
&lt;li&gt;Lead-to-customer average cost: 3.25 $ customer.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To find the optimal value of the discrimination threshold the confusion matrix and a business defined economic impact matrix will be used.&lt;/p&gt;
















&lt;figure  id=&#34;figure-conf_impact_matrices&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Confusion and Impact matrices.&#34; srcset=&#34;
               /project/2leadscoring/conf_impact_matrices_hu2a5372f174f0c48d8593bdad80cf997c_45869_8196b85d952f2bb09730219b9b93b4b4.webp 400w,
               /project/2leadscoring/conf_impact_matrices_hu2a5372f174f0c48d8593bdad80cf997c_45869_e431f1b6d3ede020357677125ec8f05b.webp 760w,
               /project/2leadscoring/conf_impact_matrices_hu2a5372f174f0c48d8593bdad80cf997c_45869_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/2leadscoring/conf_impact_matrices_hu2a5372f174f0c48d8593bdad80cf997c_45869_8196b85d952f2bb09730219b9b93b4b4.webp&#34;
               width=&#34;760&#34;
               height=&#34;231&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Confusion and Impact matrices.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;text id=&#34;conf_matrix&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Confusion matrix:&lt;/text&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Confusion matrices represent counts from predicted and actual values. The output “TN” stands for True Negative which shows the number of negative examples classified accurately. Similarly, “TP” stands for True Positive which indicates the number of positive examples classified accurately. The term “FP” shows False Positive value, i.e., the number of actual negative examples classified as positive; and “FN” means a False Negative value which is the number of actual positive examples classified as negative.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;text id=&#34;conf_matrix&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Impact matrix:&lt;/text&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It accounts for the economic impact represented by each of the components of the confusion matrix. The output “ITN” stands for Impact of True Negative which shows the economic impact of not to carry out any commercial actions on those leads that were not going to buy the product. Similarly, “ITP” stands for Impact of True Positive which indicates the net profit obtained from commercial actions on customers who end up buying the course. The term “IFP” shows Impact of False Positive value, i.e., the opportunity cost of not having carried out commercial actions on leads who would have become customers. Finally, “IFN” means a Impact of False Negative value which represent the economic cost of commercial actions carried out on a lead that finally does not buy the company&amp;rsquo;s product.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By calculating the confusion matrix multiplying it by the economic impact matrix for each of the possible values of the discrimination threshold it will be possible to assess which discrimination threshold maximises the resulting function and thus company&amp;rsquo;s ROI.&lt;/p&gt;
















&lt;figure  id=&#34;figure-disc_threshold&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Expected value for each discrimination threshold.&#34; srcset=&#34;
               /project/2leadscoring/disc_threshold_huf5d1eb742251dad3e186d9bc658961d1_56804_5dcddf4de42d1b1a9ab34aaba7b75609.webp 400w,
               /project/2leadscoring/disc_threshold_huf5d1eb742251dad3e186d9bc658961d1_56804_5d62f27fe78579aa7daa671a6a42a420.webp 760w,
               /project/2leadscoring/disc_threshold_huf5d1eb742251dad3e186d9bc658961d1_56804_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/2leadscoring/disc_threshold_huf5d1eb742251dad3e186d9bc658961d1_56804_5dcddf4de42d1b1a9ab34aaba7b75609.webp&#34;
               width=&#34;760&#34;
               height=&#34;475&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Expected value for each discrimination threshold.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;In this case, the discrimination threshold value that provides a higher return on investment for the company is 0.07.&lt;/p&gt;
&lt;h3 id=&#34;threshold&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Model results: KPIs improvements achieved&lt;/h3&gt;
&lt;p&gt;Finally, the model has been tested on a batch of 1848 leeds never seen before by the model. By applying the developed lead scoring predictive model, the company has been able to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Increase its sales profit by 2.7%.&lt;/li&gt;
&lt;li&gt;Save 36.8% of the amount of money lost due to the management of low quality leads.&lt;/li&gt;
&lt;li&gt;Increase lead-to-customer conversion rate from 38% to 50%.&lt;/li&gt;
&lt;li&gt;Save 23% of time spent by employees on managing leads.&lt;/li&gt;
&lt;/ol&gt;
















&lt;figure  id=&#34;figure-scoring_results&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;KPIs improvements achieved by applying the predictive lead scoring model.&#34; srcset=&#34;
               /project/2leadscoring/scoring_results_hu3bc1e4921c31fabe7b21cfd4fc58ea70_24598_c0d66e27d704928eee536d23669aab53.webp 400w,
               /project/2leadscoring/scoring_results_hu3bc1e4921c31fabe7b21cfd4fc58ea70_24598_23ce326851b6bbb3dec5560609dfe169.webp 760w,
               /project/2leadscoring/scoring_results_hu3bc1e4921c31fabe7b21cfd4fc58ea70_24598_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/2leadscoring/scoring_results_hu3bc1e4921c31fabe7b21cfd4fc58ea70_24598_c0d66e27d704928eee536d23669aab53.webp&#34;
               width=&#34;565&#34;
               height=&#34;201&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      KPIs improvements achieved by applying the predictive lead scoring model.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;model-deployment&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;&lt;a href=&#34;https://github.com/pedrocorma/lead-scoring-and-segmentation&#34;&gt;Model deployment - Web app&lt;/a&gt;&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;In order to get the most value out of the developed machine learning model, it is important to seamlessly deploy it into production so employees can start using them to make practical decisions.&lt;/p&gt;
&lt;p&gt;To this end, a prototype web application has been designed. This will collect, on the one hand, the internal data that the company has for each lead (time spent on the website, number of visits, origin&amp;hellip;) and on the other hand, the information provided by the lead itself through a web form.&lt;/p&gt;
&lt;ul class=&#34;cta-group&#34;&gt;
  
  &lt;li&gt;
    &lt;a href=&#34;https://p-03-notebooks03-systemapp-lead-scoringapp-lead-scoring-omfnil.streamlitapp.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34; class=&#34;btn btn-primary px-3 py-3&#34;&gt;Launch Lead Score Analyzer Web App!&lt;/a&gt;
  &lt;/li&gt;
  
  
&lt;/ul&gt;

















&lt;figure  id=&#34;figure-web_app_1&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Lead scoring web app: data introduction.&#34; srcset=&#34;
               /project/2leadscoring/web_app_1_hub84e6a279cc42f8c4a43f8d08c386b83_280840_74acbfa1dc75b1b9cfde15a3809287a6.webp 400w,
               /project/2leadscoring/web_app_1_hub84e6a279cc42f8c4a43f8d08c386b83_280840_2798e33e9ee251321881de432e349818.webp 760w,
               /project/2leadscoring/web_app_1_hub84e6a279cc42f8c4a43f8d08c386b83_280840_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/2leadscoring/web_app_1_hub84e6a279cc42f8c4a43f8d08c386b83_280840_74acbfa1dc75b1b9cfde15a3809287a6.webp&#34;
               width=&#34;760&#34;
               height=&#34;403&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Lead scoring web app: data introduction.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Once the data has been entered, by clicking on &amp;ldquo;CALCULATE SCORING&amp;rdquo; button, the application will run the machine learning model on the data and will return both the lead score and the recommendation as to whether or not it is profitable to carry out commercial actions on the lead.&lt;/p&gt;
















&lt;figure  id=&#34;figure-web_app_2&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Lead scoring web app: results.&#34; srcset=&#34;
               /project/2leadscoring/web_app_2_hua9416f85978bda416f0298b2baedf7f3_309845_cb5c0cc02fbd3105f8ae7054c6a12527.webp 400w,
               /project/2leadscoring/web_app_2_hua9416f85978bda416f0298b2baedf7f3_309845_953c718db0591ba71316cb04a34a745b.webp 760w,
               /project/2leadscoring/web_app_2_hua9416f85978bda416f0298b2baedf7f3_309845_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/2leadscoring/web_app_2_hua9416f85978bda416f0298b2baedf7f3_309845_cb5c0cc02fbd3105f8ae7054c6a12527.webp&#34;
               width=&#34;760&#34;
               height=&#34;403&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Lead scoring web app: results.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Credit Risk Scoring</title>
      <link>https://pedrocorma.github.io/project/1riskscoring/</link>
      <pubDate>Sun, 08 May 2022 00:00:00 +0000</pubDate>
      <guid>https://pedrocorma.github.io/project/1riskscoring/</guid>
      <description>&lt;h2 id=&#34;table-of-contents&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Table of contents&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#objectives&#34;&gt;Objectives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#understanding-the-problem&#34;&gt;Understanding the problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#project-design&#34;&gt;Project design&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-quality&#34;&gt;Data quality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#eda&#34;&gt;Exploratory data analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feature-transformation&#34;&gt;Feature transformation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modelling&#34;&gt;Modelling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-deployment&#34;&gt;Model deployment - Web app&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;br&gt;
&lt;ul class=&#34;cta-group&#34;&gt;
  
  &lt;li&gt;
    &lt;a href=&#34;https://03-notebooks03-systemapp-risk-scoring-deploymentapp-ri-cv1jfo.streamlitapp.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34; class=&#34;btn btn-primary px-3 py-3&#34;&gt;Launch Credit Risk Analyzer Web App!&lt;/a&gt;
  &lt;/li&gt;
  
  
&lt;/ul&gt;

&lt;div style=&#34;text-align: justify; font-size:0.9rem&#34;;&gt;
&lt;p&gt;
&lt;h2 id=&#34;introduction&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Introduction&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;The client is an online platform which specialises in lending various types of loans to urban customers. Borrowers can easily access lower interest rate loans through a fast online interface.&lt;/p&gt;
&lt;p&gt;When the company receives a loan application, the company has to make a decision for loan approval based on the applicant’s profile. Like most other lending companies, lending loans to &amp;lsquo;risky&amp;rsquo; applicants is the largest source of financial loss. The company aims to identify such &amp;lsquo;risky&amp;rsquo; applicants and their associated loan’s expected loss in order to utilise this knowledge for managing its economic capital, portfolio and risk assessment.&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;em&gt;Notes:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This article presents an overview of the development process followed in the project.&lt;/li&gt;
&lt;li&gt;Source code can be found &lt;a href=&#34;https://github.com/pedrocorma/credit-risk-scoring&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Feel free to test the developed credit risk analyzer web application &lt;a href=&#34;https://03-notebooks03-systemapp-risk-scoring-deploymentapp-ri-cv1jfo.streamlitapp.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;objectives&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Objectives&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Creating an advanced analytical asset based on machine learning predictive models to estimate the expected financial loss of each new customer-loan binomial.&lt;/p&gt;
&lt;h2 id=&#34;understanding-the-problem&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Understanding the problem&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Credit risk is associated with the possibility of a client failing to meet contractual obligations, such as mortgages, credit card debts, and other types of loans.&lt;/p&gt;
&lt;p&gt;Minimizing the risk of default is a major concern for financial institutions. For this reason, commercial and investment banks, venture capital funds, asset management companies and insurance firms are increasingly relying on technology to predict which clients are more prone to stop honoring their debts. Accounting for credit risk in the entire portfolio of instruments must consider the likelihood of future impairment and is commonly measured through expected loss and lifetime expected credit loss. To comply with IFRS 9 or CECL, risk managers need to calculate the expected credit loss on the portfolio of financial instruments over the lifetime of the portfolio.&lt;/p&gt;
&lt;p&gt;Machine Learning models have been helping these companies to improve the accuracy of their credit risk analysis, providing a scientific method to identify potential debtors in advance.&lt;/p&gt;
&lt;h2 id=&#34;project-design&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Project design&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;methodology&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Methodology&lt;/h3&gt;
In order to estimate the expected loss (EL) associated to a certain loan application, three essential risk parameters have been considered:
&lt;ul&gt;
&lt;li&gt;&lt;text id=&#34;pd&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Probability of default (PD):&lt;/text&gt; is a measure of credit rating that is assigned internally to each customer with the aim of estimating the probability that a given customer will default.&lt;/li&gt;
&lt;li&gt;&lt;text id=&#34;pd&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Loss given default (LGD):&lt;/text&gt; is another of the key metrics used in quantitative risk analysis. It is defined as the percentage risk of exposure that is not expected to be recovered in the event of default.&lt;/li&gt;
&lt;li&gt;&lt;text id=&#34;pd&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Exposure at default (EAD):&lt;/text&gt; is defined as the percentage of outstanding debt at the time of default.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Three predictive machine learning models will be developed to estimate these risk parameters, and their predictions will be combined to estimate the expected loss of each loan transaction as follows:&lt;/p&gt;
&lt;p&gt;
$$
EL\,(\$) = PD (\%) \cdot P (\$) \cdot EAD (\%) \cdot LGD (\%)
$$

where P is the loan principal (the amount of money the borrower wishes to apply for).&lt;/p&gt;
















&lt;figure  id=&#34;figure-3models&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;EL estimation by combining predictive machine learning models for PD, EAD and LGD.&#34; srcset=&#34;
               /project/1riskscoring/3models_hu3b0e3bc874e2e1ac0eb03df73e56d955_164736_d67f3d7b3b2587889aeee6a90b229e74.webp 400w,
               /project/1riskscoring/3models_hu3b0e3bc874e2e1ac0eb03df73e56d955_164736_e115f3f52dea9d78aa9f823e17df5003.webp 760w,
               /project/1riskscoring/3models_hu3b0e3bc874e2e1ac0eb03df73e56d955_164736_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/1riskscoring/3models_hu3b0e3bc874e2e1ac0eb03df73e56d955_164736_d67f3d7b3b2587889aeee6a90b229e74.webp&#34;
               width=&#34;760&#34;
               height=&#34;473&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      EL estimation by combining predictive machine learning models for PD, EAD and LGD.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Note that the model for estimating the probability of default will be developed using a logistic regression algorithm. &amp;lsquo;Black box algorithms&amp;rsquo; are not suitable in regulated financial services as their lack of interpretability and auditability could become a macro-level risk and in some cases, the law itself may dictate a degree of explainability. To overcome this problem, a highly explainable AI model, such as logistic regression, which provide details or reasons to make the functioning of AI clear or easy to understand, will be applied.&lt;/p&gt;
&lt;p&gt;For the case of the exposure at default and loss given default models, different combinations of algorithms (Ridge, Lasso, LightGBM&amp;hellip;) and hyperparameters have been tested to find those with the best performance. LightGBM algorithm  was finally choosen in both cases.&lt;/p&gt;
&lt;h3 id=&#34;entities-and-data&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Entities and data&lt;/h3&gt;
&lt;p&gt;The data under analysis is available at &lt;a href=&#34;https://www.lendingclub.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lending Club website&lt;/a&gt; and contains information collected by the company regarding two main entities:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;text id=&#34;borrowers&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Borrowers:&lt;/text&gt; There are features in the dataset capturing information on the applicant profile, i.e. applicant employment history, number of mortgages and credit lines, annual incomes and other personal information.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;text id=&#34;loan&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Loans:&lt;/text&gt; The rest of the features provide information on the loan such as loan amount, loan interest rate, loan status (i.e. whether the loan is current or in default), loan tenor (either 36 or 60 months) between others.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;data-quality&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Data quality&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;In this stage of the project, general data quality correction processes have been applied, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Feature renaming&lt;/li&gt;
&lt;li&gt;Feature type correction&lt;/li&gt;
&lt;li&gt;Elimination of features with unique values&lt;/li&gt;
&lt;li&gt;Nulls imputation&lt;/li&gt;
&lt;li&gt;Outliers management&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The entire process can be consulted in detail &lt;a href=&#34;https://github.com/pedrocorma/credit-risk-scoring/blob/main/03_Notebooks/02_Development/02_Data%20Quality.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;eda&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Exploratory data analysis&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;The aim of this stage of the project is to discover trends, patterns, and to check assumptions with the help of statistical summary and graphical representations. Complete analysis can be found &lt;a href=&#34;https://github.com/pedrocorma/credit-risk-scoring/blob/main/03_Notebooks/02_Development/03_EDA.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In order to guide the process, a series of seed questions were posed to serve as a basis for developing and deepening the analysis of the different features.&lt;/p&gt;
&lt;h3 id=&#34;seed_questions&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Seed questions&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Regarding Borrowers:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;text id=&#34;q1&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Q1:&lt;/text&gt; What are the most frequent professions of clients applying for loans?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;text id=&#34;q2&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Q2:&lt;/text&gt; How is the score feature assigned by the company to each applicant performing?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;text id=&#34;q3&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Q3:&lt;/text&gt; Can different customer behaviour profiles be distinguished with regard to the way they use their credit cards?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Regarding Loans&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;text id=&#34;q4&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Q4:&lt;/text&gt; Regarding the percentage of late payments and carged-offs, are there differences between 36 and 60 month loans?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;text id=&#34;q5&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Q5:&lt;/text&gt; Can specific loan purposes be identified as more likely to default than others?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some of the analyses carried out in this data exploration phase are shown below.&lt;/p&gt;
















&lt;figure  id=&#34;figure-eda_categorical&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Example of the analysis of some of the categorical features present in the dataset.&#34; srcset=&#34;
               /project/1riskscoring/eda_categorical1_hu41d11ac6eeb58e4956db599729c8b7cf_85293_dc6550207549dff3f166c1a31368f2f2.webp 400w,
               /project/1riskscoring/eda_categorical1_hu41d11ac6eeb58e4956db599729c8b7cf_85293_5c6bb8432f05eca0f827d7b60365579e.webp 760w,
               /project/1riskscoring/eda_categorical1_hu41d11ac6eeb58e4956db599729c8b7cf_85293_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/1riskscoring/eda_categorical1_hu41d11ac6eeb58e4956db599729c8b7cf_85293_dc6550207549dff3f166c1a31368f2f2.webp&#34;
               width=&#34;760&#34;
               height=&#34;384&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Example of the analysis of some of the categorical features present in the dataset.
    &lt;/figcaption&gt;&lt;/figure&gt;
















&lt;figure  id=&#34;figure-eda_topjobs&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The most 15 employments aforded a loan.&#34; srcset=&#34;
               /project/1riskscoring/eda_topjobs_hud9a277cc8d1c3f6a7a912f31978ae371_39369_697d17a3e98e45d81d9fbf11f049d6c9.webp 400w,
               /project/1riskscoring/eda_topjobs_hud9a277cc8d1c3f6a7a912f31978ae371_39369_0a541dcb6b2175dc1dd9c1adc3b6ce0d.webp 760w,
               /project/1riskscoring/eda_topjobs_hud9a277cc8d1c3f6a7a912f31978ae371_39369_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/1riskscoring/eda_topjobs_hud9a277cc8d1c3f6a7a912f31978ae371_39369_697d17a3e98e45d81d9fbf11f049d6c9.webp&#34;
               width=&#34;760&#34;
               height=&#34;383&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The most 15 employments aforded a loan.
    &lt;/figcaption&gt;&lt;/figure&gt;
















&lt;figure  id=&#34;figure-eda_numerical&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Example of the analysis of some of the numerical features present in the dataset.&#34; srcset=&#34;
               /project/1riskscoring/eda_numerical_hu35dc77470ce96a66089262f8d4d8ccc8_230355_97999403e1662dcde2b870fe45d474cb.webp 400w,
               /project/1riskscoring/eda_numerical_hu35dc77470ce96a66089262f8d4d8ccc8_230355_d82977285833f76d66d4055a0ea29254.webp 760w,
               /project/1riskscoring/eda_numerical_hu35dc77470ce96a66089262f8d4d8ccc8_230355_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/1riskscoring/eda_numerical_hu35dc77470ce96a66089262f8d4d8ccc8_230355_97999403e1662dcde2b870fe45d474cb.webp&#34;
               width=&#34;760&#34;
               height=&#34;548&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Example of the analysis of some of the numerical features present in the dataset.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h3 id=&#34;eda_insights&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Insights&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Borrowers:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Borrowers with poorer credit scores tend to borrow larger amounts and have lower annual incomes than clients with higher credit scores, thus paying higher monthly installments and higher interest rates.&lt;/li&gt;
&lt;li&gt;One third of all customers have been employed for more than 10 years. The job title of most clients is unknown. Of the clients who do provide this information, the top three most frequent jobs are &amp;lsquo;Teacher&amp;rsquo;, &amp;lsquo;Manager&amp;rsquo; and &amp;lsquo;Owner&amp;rsquo;.&lt;/li&gt;
&lt;li&gt;The score feature appears to be predictive of loan status: the percentage of loans charged off increases as the borrower&amp;rsquo;s credit score worsens while the percentage of fully paid loans increases as the borrower&amp;rsquo;s credit score increases.&lt;/li&gt;
&lt;li&gt;Three main groups can be clearly distinguished: those borrowers who used less than 20% of the credit available on their credit card, another group of borrowers have used between 20 and 80 percent of the available credit on their credit card, and a last group of borrower who have used more than 80% of their available credit on their credit card.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Loans:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In general, 60-month loans tend to have a higher percentage of late payments and charge-offs.&lt;/li&gt;
&lt;li&gt;The percentage of loans charged off for &amp;lsquo;moving&amp;rsquo; and &amp;lsquo;small business&amp;rsquo; purposes is slightly higher (16%-17%) than the average for the rest of loan purposes (around 11%).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;feature-transformation&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Feature transformation&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;At this stage of the project, different feature transformation techniques will be applied to adapt them to the requirements of the algorithms that will be used during the modelling phase. Detailed process can be found &lt;a href=&#34;https://github.com/pedrocorma/credit-risk-scoring/blob/main/03_Notebooks/02_Development/04_Feature%20Transformation.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As discussed prevously, three different models will be developed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PD: Probability of default model.&lt;/li&gt;
&lt;li&gt;EAD: Exposure at default model.&lt;/li&gt;
&lt;li&gt;LGD: Loss given default model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In all cases, categorical features have to be transformed into numerical features. Ordinal encoding, one hot encoding and target encoding techniques will be employed for this purpose. Note that as target encoding process is target-dependent, 3 different transformations must be carried out, one for each of the models to be developed.&lt;/p&gt;
&lt;p&gt;Regarding continuous features, given that PD model will be implemented using a logistic regression algorithm (as discussed in previous stages of the project), Gaussian normalisation processes will be applied.&lt;/p&gt;
&lt;p&gt;Finally, feature rescaling processes will be applied to transform all features in the dataset to a shared 0-1 scale.&lt;/p&gt;
&lt;h3 id=&#34;targets&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Defining and creating targets&lt;/h3&gt;
The second major objective of this section is the definition and creation of the target for each of the models to be developed.
&lt;br&gt;
&lt;/br&gt;
&lt;text id=&#34;pd&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Probability of default (PD):&lt;/text&gt;
&lt;p&gt;The purpose of this model will be to predict the probability that a given customer will default. In the context of this project, a default will be defined as any delay in the payment of loan installments of more than 90 days.&lt;/p&gt;
















&lt;figure  id=&#34;figure-pdtarget&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Value count of loan status categories.&#34; srcset=&#34;
               /project/1riskscoring/pdtarget_hu1fa0e3af518cdf839e7052b797fe592d_13449_04dd7f379ef5da60d4c09435b9073d71.webp 400w,
               /project/1riskscoring/pdtarget_hu1fa0e3af518cdf839e7052b797fe592d_13449_70e18ecbcd0b2280b71165460017fe4d.webp 760w,
               /project/1riskscoring/pdtarget_hu1fa0e3af518cdf839e7052b797fe592d_13449_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/1riskscoring/pdtarget_hu1fa0e3af518cdf839e7052b797fe592d_13449_04dd7f379ef5da60d4c09435b9073d71.webp&#34;
               width=&#34;760&#34;
               height=&#34;165&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Value count of loan status categories.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;During the exploratory data analysis phase, it was found that &amp;lsquo;Charged off&amp;rsquo; and &amp;lsquo;Does not meet the credit policy. Status:Charged Off&amp;rsquo; loan status categories had to be considered as default as certain amounts had been recovered in them. Obviously, &amp;lsquo;Default&amp;rsquo; category should also be considered as default. By other hand, &amp;lsquo;Current&amp;rsquo;, &amp;lsquo;Fully Paid&amp;rsquo;, &amp;lsquo;In Grace Period&amp;rsquo; and &amp;lsquo;Does not meet the credit policy. Status:Fully Paid&amp;rsquo; categoires will clearly not be considered as non-payments.&lt;/p&gt;
&lt;p&gt;Regarding &amp;lsquo;Late (31-120 days)&amp;rsquo; category and due to the selected criterion of considering &amp;gt;90 days = default, a decision should be made whether to include this category as default or not. Given that no additional information is available, it has been decided not to consider this category as default as 66% of its range is below 90 days&lt;/p&gt;
&lt;p&gt;Taking into account all of the above, a binary target is created, where class 1 indicates the cases considered as default.&lt;/p&gt;
&lt;p&gt;&lt;text id=&#34;lgd&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Loss given default (LGD):&lt;/text&gt;&lt;/p&gt;
&lt;p&gt;The objective of this model is to predict the percentage of the loan that a given borrower has not yet repaid when a default occurs. The target for this model can thus be defined as:&lt;/p&gt;

$$
\text{target}_{\text{ead}} = \dfrac{\text{Amount to be paid}}{\text{Loan amount}} = \dfrac{\text{Loan amount} - \text{Amortised amount}}{\text{Loan amount}}
$$

&lt;p&gt;&lt;text id=&#34;ead&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Exposure at default (EAD):&lt;/text&gt;&lt;/p&gt;
&lt;p&gt;The objective of this model is to predict the percentage of the principal that will not be possible to recover from a loan that has been defaulted on. Therefore, the target for this model will be defined as:&lt;/p&gt;

$$
\text{target}_{\text{lgd}} =
    \begin{cases}
      \dfrac{\text{Recovered amount}}{\text{Amount to be paid}} &amp; if \, \text{Amount to be paid} &gt; 1  \\
      0 &amp; if \, \text{Amount to be paid} = 0
    \end{cases}
$$

&lt;h2 id=&#34;modelling&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Modelling&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;pdmodel&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Probability of default model (PD)&lt;/h3&gt;
&lt;p&gt;As discussed in the design stage of the project, probability of default (PD) model will be based on the logistic regression algorithm. This stage aims to find the optimal combination of hyperparameters for this algorithm arquitecture.&lt;/p&gt;
















&lt;figure  id=&#34;figure-correlationspd&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;PD model: Feature correlations.&#34; srcset=&#34;
               /project/1riskscoring/correlationspd_hue56e4ba0e801336709abc91c9ed7eff4_38273_caec5243b25ffa27372bff6a89da3672.webp 400w,
               /project/1riskscoring/correlationspd_hue56e4ba0e801336709abc91c9ed7eff4_38273_afdddcd1c3a1b8c5ebefaa3330676aa8.webp 760w,
               /project/1riskscoring/correlationspd_hue56e4ba0e801336709abc91c9ed7eff4_38273_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/1riskscoring/correlationspd_hue56e4ba0e801336709abc91c9ed7eff4_38273_caec5243b25ffa27372bff6a89da3672.webp&#34;
               width=&#34;760&#34;
               height=&#34;583&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      PD model: Feature correlations.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;As already detected during the exploratory analysis phase, there is a high correlation between scoring and interest_rate features, as well as between loan amount and the monthly installments. After an iterative modelling process, it is found that the scoring and loan_amount features are more predictive than interest_rate and installment, so only the first two will be used in order to avoid these high correlations.&lt;/p&gt;
&lt;p&gt;The remaining correlations are mainly due to the one hot encoding process. Lasso-regularised algorithm settings will preferably be selected to mitigate these effects.&lt;/p&gt;
&lt;p&gt;Once the hyperparameter optimisation process has been carried out, in which different penalty strengths and strategies have been tested (l1, l2, elasticnet, none), it is found that the best parameterisation corresponds to a &lt;text id=&#34;lasso&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;logistic regression with lasso regularisation&lt;/text&gt;, which obtains an ROC-AUC value of 0.7 on the test data.&lt;/p&gt;
















&lt;figure  id=&#34;figure-auc_pd&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;PD model: ROC curve.&#34; srcset=&#34;
               /project/1riskscoring/auc_pd_hu64a32dfcf16d9fcf65b354ebc41b7054_28274_71d81e3222e271fdfdc7aa15efdf031d.webp 400w,
               /project/1riskscoring/auc_pd_hu64a32dfcf16d9fcf65b354ebc41b7054_28274_e6e74255c80f2b6f8484b39e7327668f.webp 760w,
               /project/1riskscoring/auc_pd_hu64a32dfcf16d9fcf65b354ebc41b7054_28274_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/1riskscoring/auc_pd_hu64a32dfcf16d9fcf65b354ebc41b7054_28274_71d81e3222e271fdfdc7aa15efdf031d.webp&#34;
               width=&#34;506&#34;
               height=&#34;361&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      PD model: ROC curve.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;On the other hand, through the coefficient analysis of the trained logistic regression model, it is observed that the features that most strongly determine the customer&amp;rsquo;s probability of default are the customer&amp;rsquo;s credit score, employment, number of credit lines and annual income.&lt;/p&gt;
















&lt;figure  id=&#34;figure-feature_importances_pd&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;PD model: Feature importances.&#34; srcset=&#34;
               /project/1riskscoring/feature_importances_pd_hu57c70ed29f2d132f89ae567b134661ec_78476_703a9293d7484652b3aa7af190092356.webp 400w,
               /project/1riskscoring/feature_importances_pd_hu57c70ed29f2d132f89ae567b134661ec_78476_d70ee16448e36d2fbb3f4f44100a1d03.webp 760w,
               /project/1riskscoring/feature_importances_pd_hu57c70ed29f2d132f89ae567b134661ec_78476_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/1riskscoring/feature_importances_pd_hu57c70ed29f2d132f89ae567b134661ec_78476_703a9293d7484652b3aa7af190092356.webp&#34;
               width=&#34;760&#34;
               height=&#34;381&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      PD model: Feature importances.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Note: complete development process of the PD model can be consulted &lt;a href=&#34;https://github.com/pedrocorma/credit-risk-scoring/blob/main/03_Notebooks/02_Development/05_Supervised%20Classification%20Modelling%20PD.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;lgdmodel&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Loss given default model (LGD)&lt;/h3&gt;
&lt;p&gt;At this stage different combinations of algorithms (Ridge, Lasso, LightGBM) and hyperparameters have been tested to find those with the best performance. It is found that &lt;text id=&#34;lasso&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;LightGBM&lt;/text&gt; architecture is the one that is performing best.&lt;/p&gt;
















&lt;figure  id=&#34;figure-correlationslgd&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;LGD model: Correlation between model predictions and actual values.&#34; srcset=&#34;
               /project/1riskscoring/correlationslgd_huc2051ece34ec6f433edecd212ffc4915_100731_5bd82ad4891df16e0d73e79693dcff80.webp 400w,
               /project/1riskscoring/correlationslgd_huc2051ece34ec6f433edecd212ffc4915_100731_6f4a4f94835becd0997c7b9f8a02bbcc.webp 760w,
               /project/1riskscoring/correlationslgd_huc2051ece34ec6f433edecd212ffc4915_100731_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/1riskscoring/correlationslgd_huc2051ece34ec6f433edecd212ffc4915_100731_5bd82ad4891df16e0d73e79693dcff80.webp&#34;
               width=&#34;760&#34;
               height=&#34;270&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      LGD model: Correlation between model predictions and actual values.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;As can be seen, the error made by the model in predicting the level of not amortised loan when default occurs is high. Nevertheless, it should be noted that errors in this type of risk acquisition models are generally significantly higher than those in behavioural models, marketing, customer management, etc., as much less customer information is available when running the model.&lt;/p&gt;
&lt;p&gt;In the same vein, it should also be noted that both defaulting and non-defaulting borrowers are being modelled, as this information is not available for a new customer. Therefore, on many occasions the model will be trying to predict the exposure at default of borrowers who are unlikely to default, which also explains the level of errors obtained in the modelling.&lt;/p&gt;
















&lt;figure  id=&#34;figure-densitylgd&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;LGD model: Checking the similarity between the density function of the model predictions and the actual value of the target.&#34; srcset=&#34;
               /project/1riskscoring/densitylgd_hu308bd7f0046d97e0e33164782a8dd4e9_19233_bc60978c8ba2c5564b153e0651f0a1d7.webp 400w,
               /project/1riskscoring/densitylgd_hu308bd7f0046d97e0e33164782a8dd4e9_19233_c7f32e8188f5c64b05450c54a2a538ab.webp 760w,
               /project/1riskscoring/densitylgd_hu308bd7f0046d97e0e33164782a8dd4e9_19233_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/1riskscoring/densitylgd_hu308bd7f0046d97e0e33164782a8dd4e9_19233_bc60978c8ba2c5564b153e0651f0a1d7.webp&#34;
               width=&#34;599&#34;
               height=&#34;303&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      LGD model: Checking the similarity between the density function of the model predictions and the actual value of the target.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;It can be seen that in reality (ead_true) three large groups can be distinguished, a majority group of borrowers whose exposure to default is zero, a second group with intermediate exposures (0.25-0.75), and a last group where all those borrowers with greater exposure to default are concentrated.&lt;/p&gt;
&lt;p&gt;Model&amp;rsquo;s predictions tend towards intermediate default exposures, which leads to larger errors in predicting those borrowers with very low or very high actual default exposures.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For customers who will actually have very limited exposures at default: the model predicts that they will have some degree at exposure to default, which will lead to somewhat higher fees/interest being charged than they would be entitled to.&lt;/li&gt;
&lt;li&gt;For customers with high actual default exposures: the model will tend to predict lower than actual default exposures, so that lower fees/interest will be applied than would be the case.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, at an aggregate level from a business point of view the performance of the model is quite acceptable, as it will be covering the part of fees/interest not collected from borrowers who end up having high exposure defaults with the additional surcharges/interest charged to those customers who eventually did not have defaults, thus covering the aggregate risk of the client portfolio.&lt;/p&gt;
&lt;p&gt;Note: complete development process of the LGD model can be consulted &lt;a href=&#34;https://github.com/pedrocorma/credit-risk-scoring/blob/main/03_Notebooks/02_Development/07_Supervised%20Regression%20Modelling%20LGD.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;eadmodel&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Exposure at default model (EAD)&lt;/h3&gt;
&lt;p&gt;Similar to the process carried out in the LGD model, different parametrisations of Ridge, Lasso and LightGBM algorithms have been tested, and again &lt;text id=&#34;lasso&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;LightGBM&lt;/text&gt; architecture provides the best results.&lt;/p&gt;
&lt;p&gt;The error of this model is relatively high, which is explained for the same reasons exposed previously (as it is also a risk acquisition model with limited features available to make predictions).&lt;/p&gt;
















&lt;figure  id=&#34;figure-correlationsead&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;EAD model: Correlation between model predictions and actual values.&#34; srcset=&#34;
               /project/1riskscoring/correlationsead_hud373c96bfb8abeaf5062bef1eade60d5_4262_7da405f61f1a585e95805073412fbe55.webp 400w,
               /project/1riskscoring/correlationsead_hud373c96bfb8abeaf5062bef1eade60d5_4262_a2bd5988a49eb7a7d9b3bdec50af92e2.webp 760w,
               /project/1riskscoring/correlationsead_hud373c96bfb8abeaf5062bef1eade60d5_4262_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/1riskscoring/correlationsead_hud373c96bfb8abeaf5062bef1eade60d5_4262_7da405f61f1a585e95805073412fbe55.webp&#34;
               width=&#34;251&#34;
               height=&#34;105&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      EAD model: Correlation between model predictions and actual values.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;It can be seen that in reality (lgd_true) two large groups can be distinguished: a group of loans in which no amount is recovered, either because the borrower has not defaulted or because the borrower has defaulted but the bank has not been able to recover any amount; and a second group of loans in which it has been possible to recover the full amount, either because the borrower has amortised the entire loan or because it has been possible to recover the full amount a defaulted loan.&lt;/p&gt;
&lt;p&gt;Model&amp;rsquo;s predictions tend towards intermediate loss levels, which leads to larger errors in predicting fully recovered or lost loans.&lt;/p&gt;
















&lt;figure  id=&#34;figure-densityead&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;EAD model: Checking the similarity between the density function of the model predictions and the actual value of the target.&#34; srcset=&#34;
               /project/1riskscoring/densityead_hu574b1d95b725ef24ae5055fcd35e68d0_23044_0cddee2b17e8ff6eff75e27c33825bb5.webp 400w,
               /project/1riskscoring/densityead_hu574b1d95b725ef24ae5055fcd35e68d0_23044_4cc6526bc47d7a9deb7dee6dd2b839a7.webp 760w,
               /project/1riskscoring/densityead_hu574b1d95b725ef24ae5055fcd35e68d0_23044_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/1riskscoring/densityead_hu574b1d95b725ef24ae5055fcd35e68d0_23044_0cddee2b17e8ff6eff75e27c33825bb5.webp&#34;
               width=&#34;609&#34;
               height=&#34;303&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      EAD model: Checking the similarity between the density function of the model predictions and the actual value of the target.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;However, as presented for LGD model, the performance of the EAD model is acceptable at an aggregate level from a business point of view, as it will be covering the lost amount of loans in which the amount borrowed has been completely lost by predicting to most customers a loss level of between 25% and 75% even those who ultimately fully paid their loans, thus covering the aggregate risk of the client portfolio.&lt;/p&gt;
&lt;p&gt;Note: complete development process of the EAD model can be consulted &lt;a href=&#34;https://github.com/pedrocorma/credit-risk-scoring/blob/main/03_Notebooks/02_Development/06_Supervised%20Regression%20Modelling%20EAD.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;elmodel&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Final Expected Loss model (EL)&lt;/h3&gt;
&lt;p&gt;Once the probability of default, exposure at default and loss given default models have been developed, the expected loss (EL) for each new loan application is obtained by simply combining the predictions of these models and the principal amount of the loan as discussed in the methodology section.&lt;/p&gt;

$$
EL\,(\$) = PD (\%) \cdot P (\$) \cdot EAD (\%) \cdot LGD (\%)
$$

&lt;h2 id=&#34;model-deployment&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;&lt;a href=&#34;https://github.com/pedrocorma/lead-scoring-and-segmentation&#34;&gt;Model deployment - Web app&lt;/a&gt;&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;In order to get the most value out of the developed machine learning models, it is important to seamlessly deploy it into production so employees can start using them to make practical decisions.&lt;/p&gt;
&lt;p&gt;To this end, a prototype web application has been designed. This web app collects, on the one hand, the internal data that the company has for each client and on the other hand, the information provided by the borrower itself through a loan application.&lt;/p&gt;
&lt;br&gt;
&lt;/br&gt;
&lt;ul class=&#34;cta-group&#34;&gt;
  
  &lt;li&gt;
    &lt;a href=&#34;https://03-notebooks03-systemapp-risk-scoring-deploymentapp-ri-cv1jfo.streamlitapp.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34; class=&#34;btn btn-primary px-3 py-3&#34;&gt;Launch Credit Risk Analyzer Web App!&lt;/a&gt;
  &lt;/li&gt;
  
  
&lt;/ul&gt;

















&lt;figure  id=&#34;figure-webapp1&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Credit risk analyzer web app: data introduction.&#34; srcset=&#34;
               /project/1riskscoring/webapp1_hucfd788f871ec67fd065f58e7e0a4b760_275885_8c3cc1acb2c65d35676d9ccc7de10e37.webp 400w,
               /project/1riskscoring/webapp1_hucfd788f871ec67fd065f58e7e0a4b760_275885_4fa2f27f3bdcdc66a00ff92aa24f5146.webp 760w,
               /project/1riskscoring/webapp1_hucfd788f871ec67fd065f58e7e0a4b760_275885_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/1riskscoring/webapp1_hucfd788f871ec67fd065f58e7e0a4b760_275885_8c3cc1acb2c65d35676d9ccc7de10e37.webp&#34;
               width=&#34;760&#34;
               height=&#34;404&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Credit risk analyzer web app: data introduction.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Once the data has been entered, by clicking on “CALCULATE RISK” button, the application will run the machine learning models on the data and will return the expected loss of the loan application as well as the probability of default, loss given default and exposure at default KPIs.&lt;/p&gt;
















&lt;figure  id=&#34;figure-webapp2&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Credit risk analyzer web app: results.&#34; srcset=&#34;
               /project/1riskscoring/webapp2_hu5dd0ceaa2cac64d33ab1588d93db23ff_298794_45a8595e095c52a00806213602b375cb.webp 400w,
               /project/1riskscoring/webapp2_hu5dd0ceaa2cac64d33ab1588d93db23ff_298794_305258642fee21bde15fba8004649695.webp 760w,
               /project/1riskscoring/webapp2_hu5dd0ceaa2cac64d33ab1588d93db23ff_298794_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/1riskscoring/webapp2_hu5dd0ceaa2cac64d33ab1588d93db23ff_298794_45a8595e095c52a00806213602b375cb.webp&#34;
               width=&#34;760&#34;
               height=&#34;404&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Credit risk analyzer web app: results.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Retail Sales Forecasting</title>
      <link>https://pedrocorma.github.io/project/0forecasting/</link>
      <pubDate>Sun, 01 May 2022 00:00:00 +0000</pubDate>
      <guid>https://pedrocorma.github.io/project/0forecasting/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Note: Current documentation available on the &lt;a href=&#34;https://github.com/pedrocorma/retail-sales-forecasting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub repository&lt;/a&gt; is in Spanish. It will soon be updated to English.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;table-of-contents&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Table of contents&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#objectives&#34;&gt;Objectives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#understanding-the-problem&#34;&gt;Understanding the problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#project-design&#34;&gt;Project design&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-quality&#34;&gt;Data quality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#eda&#34;&gt;Exploratory data analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feature-transformation&#34;&gt;Feature engineering and transformations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modelling&#34;&gt;Modelling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scripts&#34;&gt;Retraining and production scripts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div style=&#34;text-align: justify;&#34;&gt;
&lt;p&gt;
&lt;h2 id=&#34;introduction&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Introduction&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;The client is a large American retailer that desires to implement a sales prediction system based on artificial intelligence algorithms.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Notes:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This article presents a technical explanation of the development process followed in the project.&lt;/li&gt;
&lt;li&gt;Source code can be found &lt;a href=&#34;https://github.com/pedrocorma/retail-sales-forecasting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;objectives&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Objectives&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Developing a set of machine learning models on a three-year-history SQL database to predict sales for the next 8 days at the store-product level using massive modelling techniques.&lt;/p&gt;
















&lt;figure  id=&#34;figure-support3&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/0forecasting/support3_hub58e4e5716bf83ae12ae762563fc86c8_343241_3dd914159386ad007f2a1952890eb8ef.webp 400w,
               /project/0forecasting/support3_hub58e4e5716bf83ae12ae762563fc86c8_343241_8bf067333aea138a1c86c533e7a39dc7.webp 760w,
               /project/0forecasting/support3_hub58e4e5716bf83ae12ae762563fc86c8_343241_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/0forecasting/support3_hub58e4e5716bf83ae12ae762563fc86c8_343241_3dd914159386ad007f2a1952890eb8ef.webp&#34;
               width=&#34;760&#34;
               height=&#34;337&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;h2 id=&#34;understanding-the-problem&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Understanding the problem&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;One of the most frequent applications of data science is sales forecasting due to its direct impact on the balance sheet.&lt;/p&gt;
















&lt;figure  id=&#34;figure-support1&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/0forecasting/support11_hu6845ff192ffc083677d62e7740ab8a73_112102_799d6c972a7bc7303f09d4bc0abd2003.webp 400w,
               /project/0forecasting/support11_hu6845ff192ffc083677d62e7740ab8a73_112102_4f0d542abd69818e94d70596b0ed4574.webp 760w,
               /project/0forecasting/support11_hu6845ff192ffc083677d62e7740ab8a73_112102_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/0forecasting/support11_hu6845ff192ffc083677d62e7740ab8a73_112102_799d6c972a7bc7303f09d4bc0abd2003.webp&#34;
               width=&#34;540&#34;
               height=&#34;529&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt;Sales forecasting is aimed at improving the following processes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;text id=&#34;s1&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Supplier relationship management:&lt;/text&gt; by having the prediction of customer demand in numbers, it’s possible to calculate how many products to order, making it easy for managers to decide whether the company needs new supply chains or to reduce the number of suppliers.&lt;/li&gt;
&lt;li&gt;&lt;text id=&#34;s2&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Customer relationship management:&lt;/text&gt; customers planning to buy something expect the products they want to be available immediately. Demand forecasting allows to predict which categories of products need to be purchased in the next period from a specific store location. This improves customer satisfaction and commitment to the brand.&lt;/li&gt;
&lt;li&gt;&lt;text id=&#34;s3&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Order fulfillment and logistics:&lt;/text&gt; sales forecasting features optimising supply chains. This means that at the time of order, the product will be more likely to be in stock, and unsold goods won’t occupy prime retail space.&lt;/li&gt;
&lt;li&gt;&lt;text id=&#34;s4&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Marketing campaigns:&lt;/text&gt; forecasting is often used to adjust ads and marketing campaigns and can influence the number of sales. This is one of the use cases of machine learning in marketing. Sophisticated machine learning forecasting models can take marketing data into account as well.&lt;/li&gt;
&lt;li&gt;&lt;text id=&#34;s4&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Manufacturing flow management:&lt;/text&gt; being part of the ERP, the time series-based demand forecasting predicts production needs based on how many goods will eventually be sold.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To this end, traditional sales forecasting methods have been tried and tested for decades. With Artificial Intelligence development, they are now complemented by modern forecasting methods using Machine Learning. Machine learning techniques allows for predicting the amount of products/services to be purchased during a defined future period. In this case, a software system can learn from data for improved analysis. Compared to traditional demand forecasting methods, a machine learning approach allows to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Accelerate data processing speed&lt;/li&gt;
&lt;li&gt;Automate forecast updates based on the recent data&lt;/li&gt;
&lt;li&gt;Analyze more data&lt;/li&gt;
&lt;li&gt;Identify hidden patterns in data&lt;/li&gt;
&lt;li&gt;Increase adaptability to changes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Massive and scalable machine learning modelling techniques will be used to approach the present project.&lt;/p&gt;
&lt;h2 id=&#34;project-design&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Project design&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;entities-and-data&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Project scope, entities and data&lt;/h3&gt;
This project has been developed based on the data contained in a three-year-history SQL database of a large American retailer. Due to the computational limitations of the equipment available for its development, the scope of the project has been limited to the sales forecasting of ten products belonging to a single category (food) in two different stores. However, the developed system is fully scalable to predict sales of more products, categories and stores by simply changing their respective parameters.
















&lt;figure  id=&#34;figure-support2&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/0forecasting/support2_hu9d0a719db83b912b897e7669b382ed37_231246_41ea1b41a215fe5a35d661c9b6e71933.webp 400w,
               /project/0forecasting/support2_hu9d0a719db83b912b897e7669b382ed37_231246_a6ad2e2e928819bfe572af5d4eee2191.webp 760w,
               /project/0forecasting/support2_hu9d0a719db83b912b897e7669b382ed37_231246_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/0forecasting/support2_hu9d0a719db83b912b897e7669b382ed37_231246_41ea1b41a215fe5a35d661c9b6e71933.webp&#34;
               width=&#34;760&#34;
               height=&#34;356&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;h3 id=&#34;entities-and-data&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Approach to the main forecasting-related problems&lt;/h3&gt;
&lt;h4 id=&#34;p1&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;1. Hierarchical Forecasting&lt;/h4&gt;
&lt;p&gt;More often than not, time series data follows a hierarchical aggregation structure. For example, in retail, sales for a Stock Keeping Unit (SKU) at a store can roll up to different category and subcategory hierarchies. In these cases, it must be assure that the sales estimates are in agreement when rolled up to a higher level. In these scenarios, Hierarchical Forecasting is used. It is the process of generating coherent forecasts (or reconciling incoherent forecasts) that allows individual time series to be forecasted individually while still preserving the relationships within the hierarchy.&lt;/p&gt;
















&lt;figure  id=&#34;figure-hf&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Hierarchical structure in time series for store sales. Individual product sales are depicted in the lowest level (level 2) of the tree, followed by sales aggregated on the subcategory level (level 1), and finally all of the subcategory sales aggregated on the category level (level 0).&#34; srcset=&#34;
               /project/0forecasting/hf_hu2b39a5768de7359c5b17613b004cded9_65767_5589318383edba7e15b755489588d9fa.webp 400w,
               /project/0forecasting/hf_hu2b39a5768de7359c5b17613b004cded9_65767_80519a169c3701423acb1123e0da0798.webp 760w,
               /project/0forecasting/hf_hu2b39a5768de7359c5b17613b004cded9_65767_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/0forecasting/hf_hu2b39a5768de7359c5b17613b004cded9_65767_5589318383edba7e15b755489588d9fa.webp&#34;
               width=&#34;760&#34;
               height=&#34;317&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Hierarchical structure in time series for store sales. Individual product sales are depicted in the lowest level (level 2) of the tree, followed by sales aggregated on the subcategory level (level 1), and finally all of the subcategory sales aggregated on the category level (level 0).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;text id=&#34;s1&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;1.1. Problems:&lt;/text&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are different levels of hierarchies in the commercial catalog.&lt;/li&gt;
&lt;li&gt;It may be interesting to predict sales at different levels.&lt;/li&gt;
&lt;li&gt;Since the forecasts are probabilistic, predictions at different levels will not match exactly.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;text id=&#34;s1&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;1.2. Solution:&lt;/text&gt;&lt;/p&gt;
&lt;p&gt;The forecasts at all of the levels must be coherent. To achieve that, there are different reconciliation approaches to combining and breaking forecasts at different levels. The most common of these methods are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;text id=&#34;s1&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Bottom-Up:&lt;/text&gt; in this method, the forecasts are carried out at the bottom-most level of the hierarchy, and then summed going up. For example, in the preceding figure, by using the bottom-up method, the time series’ for the products (level 2) are used to build forecasting models. The outputs of individual models are then summed to generate the forecast for the subcategories. For example, forecasts for &amp;lsquo;apples&amp;rsquo; and &amp;lsquo;grapes&amp;rsquo; are summed to get the forecasts for &amp;lsquo;fruits&amp;rsquo;. Finally, forecasts for all of the subcategories are summed to generate the forecasts for &amp;lsquo;food&amp;rsquo; category.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;text id=&#34;s1&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Top-down:&lt;/text&gt; in top-down approaches, the forecast is first generated for the top level (&amp;lsquo;Food&amp;rsquo; in the preceding figure) and then disaggregated down the hierarchy. Disaggregate proportions are used in conjunction with the top level forecast to generate forecasts at the bottom level of the hierarchy. There are multiple methods to generate these disaggregate proportions, such as average historical proportions, proportions of the historical averages, and forecast proportions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;text id=&#34;s1&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Middle-out:&lt;/text&gt; in this method, forecasts are first generated for all of the series at a &amp;lsquo;middle level&amp;rsquo; (for example, &amp;lsquo;meat&amp;rsquo;, &amp;lsquo;backery&amp;rsquo;, &amp;lsquo;frozen foods&amp;rsquo;, and &amp;lsquo;fruits&amp;rsquo; in the preceding figure). From these forecasts, the bottom-up approach is used to generate the aggregated forecasts for the levels above this middle level. For the levels below the middle level, a top-down approach is used.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;In this case, it has been decided to model at the lowest hierarchical level, i.e. at the store-product level.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;p2&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;2. Intermittent demand&lt;/h4&gt;
&lt;p&gt;Intermittent demand, also known as sporadic demand, comes about when a product experiences several periods of zero demand. Often in these situations, when demand occurs it is small, and sometimes highly variable in size.&lt;/p&gt;
















&lt;figure  id=&#34;figure-p2&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Intermittent demand.&#34; srcset=&#34;
               /project/0forecasting/p2_hu446d502e894fe9b8c3963a0854bf0cce_134301_38be46e010da81a16769bd50eae31965.webp 400w,
               /project/0forecasting/p2_hu446d502e894fe9b8c3963a0854bf0cce_134301_c3b7fadba1b3cc1c9a0993038071eea1.webp 760w,
               /project/0forecasting/p2_hu446d502e894fe9b8c3963a0854bf0cce_134301_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/0forecasting/p2_hu446d502e894fe9b8c3963a0854bf0cce_134301_38be46e010da81a16769bd50eae31965.webp&#34;
               width=&#34;760&#34;
               height=&#34;450&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Intermittent demand.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;text id=&#34;s1&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;2.1. Problems:&lt;/text&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The source of these zero values is unknown:
&lt;ul&gt;
&lt;li&gt;The product was in stock but no sales were made?&lt;/li&gt;
&lt;li&gt;The product was not in stock and therefore could not be sold?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;This situation generates noise and difficulties that worsen the algorithm predictions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;text id=&#34;s1&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;2.2. Solutions:&lt;/text&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The optimal way to approach this problem is to get the inventory information in addition to the sales information, thus being able to generate stock-out features that allow the algorithms to discriminate the cause of the zero values.&lt;/li&gt;
&lt;li&gt;If it is not possible to obtain inventory information and/or stock-out marks, another possible approach is to model at a higher hierarchical level, especially if the products are in very low demand.&lt;/li&gt;
&lt;li&gt;It is also possible to create synthetic features that try to identify whether or not stock-outs have occurred.&lt;/li&gt;
&lt;li&gt;Employing forecasting methods based on machine learning techniques, which are less sensitive to these problems than classical approaches.&lt;/li&gt;
&lt;li&gt;Employing more advanced methodologies such as:
&lt;ul&gt;
&lt;li&gt;Croston method and derivatives.&lt;/li&gt;
&lt;li&gt;ML models to predict the probability that a particular day&amp;rsquo;s sales will be zero.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;In the present project, there is no inventory information or stock-out marks available in the provided database, so it has been decided to generate synthetic features that collect the situations where stock-outs occur based on business rules.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;p2&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;3. There are hundreds or thousands of Stock Keeping Units (SKUs)&lt;/h4&gt;
&lt;p&gt;In real contexts and sectors such as retail, ecommerce, there are often thousands of different products for which to predict the sales level.&lt;/p&gt;
&lt;p&gt;&lt;text id=&#34;s1&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;3.1. Problem:&lt;/text&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Depending on the number of SKUs, the desired level of temporal aggregation (hourly/daily/weekly models&amp;hellip;), the amount of historical data being used, the computational resources available, etc., there may come a point where modelling process is not computationally feasible.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;text id=&#34;s1&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;3.2. Solutions:&lt;/text&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Employing the forecasting approach based on machine learning, since once the models are trained, it is much faster than the classical approach.&lt;/li&gt;
&lt;li&gt;Using faster models such as LightGBM.&lt;/li&gt;
&lt;li&gt;Modelling at a higher hierarchical level and estimate the forecast of the lower levels by applying top-down reconciliation techniques.&lt;/li&gt;
&lt;li&gt;Using big data techniques to train the models
&lt;ul&gt;
&lt;li&gt;Powerful cloud machines.&lt;/li&gt;
&lt;li&gt;Big Data clusters.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Forecasting approach based on machine learning and LightGBM tree-based algorithm architecture has been employed in the project.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;data-quality&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Data quality&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;In this stage of the project, general data quality correction processes have been applied, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Feature renaming&lt;/li&gt;
&lt;li&gt;Feature type correction&lt;/li&gt;
&lt;li&gt;Elimination of features with unique values&lt;/li&gt;
&lt;li&gt;Nulls imputation&lt;/li&gt;
&lt;li&gt;Outliers management&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The entire process can be consulted in detail &lt;a href=&#34;https://github.com/pedrocorma/retail-sales-forecasting/blob/main/03_Notebooks/02_Desarrollo/02_Calidad%20de%20Datos.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;eda&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Exploratory data analysis&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;The aim of this stage of the project is to discover trends, patterns, and to check assumptions with the help of statistical summary and graphical representations. Complete analysis can be found &lt;a href=&#34;https://github.com/pedrocorma/retail-sales-forecasting/blob/main/03_Notebooks/02_Desarrollo/03_EDA.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Example of the analysis of some of the analysis that have been conducted:&lt;/p&gt;
















&lt;figure  id=&#34;figure-exhibit1&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Price trend by product. Pricing policies vary widely by product. Promotions seem to be frequent. It would be really useful to have a calendar of promotions.&#34; srcset=&#34;
               /project/0forecasting/exhibit1_hu903b0d331ba6c81b90755f7e147351ab_57678_70ed531fa2bf501693ff76dd2e7c2e3b.webp 400w,
               /project/0forecasting/exhibit1_hu903b0d331ba6c81b90755f7e147351ab_57678_548df428fefaaae3d2b9b4a68ed1db5e.webp 760w,
               /project/0forecasting/exhibit1_hu903b0d331ba6c81b90755f7e147351ab_57678_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/0forecasting/exhibit1_hu903b0d331ba6c81b90755f7e147351ab_57678_70ed531fa2bf501693ff76dd2e7c2e3b.webp&#34;
               width=&#34;760&#34;
               height=&#34;385&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Price trend by product. Pricing policies vary widely by product. Promotions seem to be frequent. It would be really useful to have a calendar of promotions.
    &lt;/figcaption&gt;&lt;/figure&gt;
















&lt;figure  id=&#34;figure-exhibit2&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Seasonality of event type by product.&#34; srcset=&#34;
               /project/0forecasting/exhibit2_hu6d98e795402784a8a2b20f3dff3609cd_73506_03bc7e13f77ac4248a3c5e710d695e93.webp 400w,
               /project/0forecasting/exhibit2_hu6d98e795402784a8a2b20f3dff3609cd_73506_b9b9e5eb2cae507df4a37d0662f65b64.webp 760w,
               /project/0forecasting/exhibit2_hu6d98e795402784a8a2b20f3dff3609cd_73506_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/0forecasting/exhibit2_hu6d98e795402784a8a2b20f3dff3609cd_73506_03bc7e13f77ac4248a3c5e710d695e93.webp&#34;
               width=&#34;490&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Seasonality of event type by product.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;feature-transformation&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Feature engineering and transformations&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;At this stage of the project, different variable transformation techniques (one hot encoding, target encoding, etc.) have been applied to adapt them to the requirements of the algorithms that will be used during the modelling phase.&lt;/p&gt;
&lt;p&gt;On the other hand, aiming to increase the predictive capacity of the models, new features have been created, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;text id=&#34;s1&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Intermittent demand features:&lt;/text&gt; explained at project design section.&lt;/li&gt;
&lt;li&gt;&lt;text id=&#34;s1&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Lag features:&lt;/text&gt; are values at prior timesteps that are considered useful because they are created on the assumption that what happened in the past can influence or contain a sort of intrinsic information about the future.&lt;/li&gt;
&lt;li&gt;&lt;text id=&#34;s1&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Rolling window features:&lt;/text&gt; the main goal of building and using rolling window features in a time series dataset is to compute statistics on the values from a given data sample by defining a range that includes the sample itself as well as some specified number of samples before and after the sample used.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Complete source code of this stage can be found &lt;a href=&#34;https://github.com/pedrocorma/retail-sales-forecasting/blob/main/03_Notebooks/02_Desarrollo/04_Transformacion%20de%20datos.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;modelling&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Modelling&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;feature_selection&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Feature selection&lt;/h3&gt;
&lt;p&gt;At this stage of the project most predictive features have been analysed by comparing the results of three different selection methods (Recursive feature elimination, Mutual information, Permutation importance), to both improve the performance of the models and reduce the computational modelling costs. Finally, the 70 most predictive features identified by &amp;lsquo;Recursive feature elimination&amp;rsquo; method have been selected. Entire feature selection code is available &lt;a href=&#34;https://github.com/pedrocorma/retail-sales-forecasting/blob/main/03_Notebooks/02_Desarrollo/05_Preselecci%C3%B3n%20de%20variables.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
















&lt;figure  id=&#34;figure-rfe&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;70 most predictive features according to Recursive Feature Elimination method.&#34; srcset=&#34;
               /project/0forecasting/rfe_hu6db99f79e29f68b140ea2c23cc48a5f2_89212_42a96e85bfe02b86d3eb1cc0bacb2456.webp 400w,
               /project/0forecasting/rfe_hu6db99f79e29f68b140ea2c23cc48a5f2_89212_72921a02b8001153c66d74f6ef7662b4.webp 760w,
               /project/0forecasting/rfe_hu6db99f79e29f68b140ea2c23cc48a5f2_89212_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/0forecasting/rfe_hu6db99f79e29f68b140ea2c23cc48a5f2_89212_42a96e85bfe02b86d3eb1cc0bacb2456.webp&#34;
               width=&#34;514&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      70 most predictive features according to Recursive Feature Elimination method.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h3 id=&#34;one-step-one-model&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Developing a one-step forecasting model for a specific product-store combination&lt;/h3&gt;
The objective of this stage of the project is not to obtain the final models but to design the modeling process (algorithm selection, hyperparameter optimization, model evaluation...) for the minimum analysis unit (product-store), in order to verify the correct functioning of the process and to avoid possible error sources when scaling the one-step forecasting process to all product-store combinations.
&lt;br&gt;&lt;/br&gt;
&lt;text id=&#34;text&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Defining the validation strategy - Time series split cross-validation:&lt;/text&gt;
&lt;p&gt;In order to prevent overfitting and evaluate model performance in a more robust way than simple train-test, a cross validation strategy has been implemented.&lt;/p&gt;
&lt;p&gt;In the case of time series, the cross-validation is not trivial. Random samples can not be assigned to either the test set or the train set because it makes no sense to use the values from the future to forecast values in the past. In simple word future-looking must be avoided when training the models. There is a temporal dependency between observations that must be preserved during testing.&lt;/p&gt;
&lt;p&gt;The method that can be used for cross-validating the time-series model is cross-validation on a rolling basis. Start with a small subset of data for training purpose, forecast for the later data points and then checking the accuracy for the forecasted data points. The same forecasted data points are then included as part of the next training dataset and subsequent data points are forecasted.&lt;/p&gt;
















&lt;figure  id=&#34;figure-cv&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Time-series model cross-validation process.&#34; srcset=&#34;
               /project/0forecasting/cv_hu15e1325518b377fbfc1ee0b43b9d7a1f_14385_f3275b46a46e84d9cbc9594d54aacd81.webp 400w,
               /project/0forecasting/cv_hu15e1325518b377fbfc1ee0b43b9d7a1f_14385_a4106fc6e9428309674162f7c7b8b200.webp 760w,
               /project/0forecasting/cv_hu15e1325518b377fbfc1ee0b43b9d7a1f_14385_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/0forecasting/cv_hu15e1325518b377fbfc1ee0b43b9d7a1f_14385_f3275b46a46e84d9cbc9594d54aacd81.webp&#34;
               width=&#34;602&#34;
               height=&#34;276&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Time-series model cross-validation process.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;In the present project the cross validation process has been implemented using &lt;code&gt;sklearn.model_selection.TimeSeriesSplit&lt;/code&gt; class of scikit-learn package with 3 splits and 8 days as maximum training size.
&lt;br&gt;&lt;/br&gt;
&lt;text id=&#34;text&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Selecting the algorithm architecture:&lt;/text&gt;&lt;/p&gt;
&lt;p&gt;As discussed in the general design phase of the project, the algorithmic architecture used to implement the different models has been LightGBM given its favorable relationship between predictive capability and the computational time required.
&lt;br&gt;&lt;/br&gt;
&lt;text id=&#34;text&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Hyperparameter optimization:&lt;/text&gt;&lt;/p&gt;
&lt;p&gt;Different combinations of hyperparameters have been tested to find those with the best performance. Evaluation scores obtained in the tested parametrisations remains stable during cross-validation process, which is a good indicator of the stability of the model predictions.
&lt;br&gt;&lt;/br&gt;
&lt;text id=&#34;text&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Checking:&lt;/text&gt;&lt;/p&gt;
&lt;p&gt;The objective here is not to evaluate the model quality (since the predictions have been made using the training data), but simply to verify that the process works properly, that the order of magnitude of the predictions is correct and that no other anomalous behavior are detected before moving on with the project.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-1step1model&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Checking last 3 months predictions on train data of model for product &amp;#39;FOODS_3_586&amp;#39; on store &amp;#39;CA_3&amp;#39;.&#34; srcset=&#34;
               /project/0forecasting/1step1model_hucd9aa8be0e94f76c8201583f4090b90d_105647_bf519a771515b6493cc8c65463960663.webp 400w,
               /project/0forecasting/1step1model_hucd9aa8be0e94f76c8201583f4090b90d_105647_e5f6d8a8032bb6eb48d6e11ac8d000cb.webp 760w,
               /project/0forecasting/1step1model_hucd9aa8be0e94f76c8201583f4090b90d_105647_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/0forecasting/1step1model_hucd9aa8be0e94f76c8201583f4090b90d_105647_bf519a771515b6493cc8c65463960663.webp&#34;
               width=&#34;760&#34;
               height=&#34;478&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Checking last 3 months predictions on train data of model for product &amp;lsquo;FOODS_3_586&amp;rsquo; on store &amp;lsquo;CA_3&amp;rsquo;.
    &lt;/figcaption&gt;&lt;/figure&gt;
There do not seem to be any problems, so the progress of the project continues.&lt;/p&gt;
&lt;h3 id=&#34;one-step-one-model&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Generalizing the one-step forecasting model creation process&lt;/h3&gt;
&lt;p&gt;Once the model generation process has been created and polished at the individual level (product-store), in this section the code necessary to scale this process to all the products to be modeled has been developed.&lt;/p&gt;
&lt;p&gt;&lt;text id=&#34;text&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Checking:&lt;/text&gt;&lt;/p&gt;
&lt;p&gt;Again, the objective here is not to evaluate quality of the models but simply to verify that the generalization of the one-step forecasting model creation process works properly.
















&lt;figure  id=&#34;figure-1step20model&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Checking last 3 months predictions on train data of all product-store models.&#34; srcset=&#34;
               /project/0forecasting/1step20model_hu06661cc6c3eb83b0f8d317bf80d1190c_1548815_26818aabfc3427f1dd426371a90fdaa2.webp 400w,
               /project/0forecasting/1step20model_hu06661cc6c3eb83b0f8d317bf80d1190c_1548815_1491f5df30af469fd01705e5d412d329.webp 760w,
               /project/0forecasting/1step20model_hu06661cc6c3eb83b0f8d317bf80d1190c_1548815_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/0forecasting/1step20model_hu06661cc6c3eb83b0f8d317bf80d1190c_1548815_26818aabfc3427f1dd426371a90fdaa2.webp&#34;
               width=&#34;760&#34;
               height=&#34;531&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Checking last 3 months predictions on train data of all product-store models.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h3 id=&#34;one-step-one-model&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Recursive multi-Step Time Series Forecasting&lt;/h3&gt;
&lt;p&gt;At this point of the project it has been designed a scalable process of one-step forecasting model generation, thus being able to predict the next day&amp;rsquo;s sales at product-store level. However, the project&amp;rsquo;s objective is to make these predictions not only for the next day but for the following 8 days.&lt;/p&gt;
&lt;p&gt;Predicting multiple time steps into the future is called multi-step time series forecasting. There are several approaches to this problem, of which the two main ones are as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;text id=&#34;text&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Direct Multi-step Forecast Strategy:&lt;/text&gt; The direct method involves developing a separate model for each forecast time step. Having one model for each time step is an added computational and maintenance burden. On the other hand, because separate models are used, it means that there is no opportunity to model the dependencies between the predictions, such as the prediction on day 2 being dependent on the prediction in day 1, as is often the case in time series.
















&lt;figure  id=&#34;figure-direct&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Direct Multi-step Forecast strategy.&#34; srcset=&#34;
               /project/0forecasting/multistep_direct_hu19a3e408bc0c364a0ce25c0e632c2d26_390368_7cc6411da8a082f34040b369ad03423f.webp 400w,
               /project/0forecasting/multistep_direct_hu19a3e408bc0c364a0ce25c0e632c2d26_390368_d873b854f42b8478476f7888943bb09e.webp 760w,
               /project/0forecasting/multistep_direct_hu19a3e408bc0c364a0ce25c0e632c2d26_390368_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/0forecasting/multistep_direct_hu19a3e408bc0c364a0ce25c0e632c2d26_390368_7cc6411da8a082f34040b369ad03423f.webp&#34;
               width=&#34;760&#34;
               height=&#34;357&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Direct Multi-step Forecast strategy.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;text id=&#34;text&#34; style=&#39;text-align: left; color: #f76497; font-weight: normal;&#39;&gt;Recursive Multi-step Forecast:&lt;/text&gt; The recursive strategy involves using a one-step model multiple times where the prediction for the prior time step is used as an input for making a prediction on the following time step, being therefore a better approximation than the direct one with respect to computational and maintenance aspects. However, the recursive strategy allows prediction errors to accumulate such that performance may quickly degrade as the prediction time horizon increases.
















&lt;figure  id=&#34;figure-recursive&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Recursive Multi-step Forecast strategy.&#34; srcset=&#34;
               /project/0forecasting/multistep_recursive_huaad89ae5be066d461d264f358da8f454_329641_5f08b905854557007fe4258f708f6c4c.webp 400w,
               /project/0forecasting/multistep_recursive_huaad89ae5be066d461d264f358da8f454_329641_fe6b855da607bc926a0e63df32a175c3.webp 760w,
               /project/0forecasting/multistep_recursive_huaad89ae5be066d461d264f358da8f454_329641_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://pedrocorma.github.io/project/0forecasting/multistep_recursive_huaad89ae5be066d461d264f358da8f454_329641_5f08b905854557007fe4258f708f6c4c.webp&#34;
               width=&#34;760&#34;
               height=&#34;370&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Recursive Multi-step Forecast strategy.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Recursive multi-step forecasting approach has been finally implemented in order to reduce as much as possible the development and maintenance costs of the models.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Complete modelling process can be consulted &lt;a href=&#34;https://github.com/pedrocorma/retail-sales-forecasting/blob/main/03_Notebooks/02_Desarrollo/06_Modelizacion%20para%20Regresion.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;scripts&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Retraining and production scripts&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Once all recursive multi-step forecasting models have been developed, trained and evaluated properly, in this final stage of the project all the necessary processes, functions and code have been compiled in a clean and optimal way into two Python scripts:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/pedrocorma/retail-sales-forecasting/blob/main/05_Resultados/codigo_reentrenamiento.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Retraining script&lt;/a&gt;: automatically retrains all developed models with new data when necessary.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/pedrocorma/retail-sales-forecasting/blob/main/05_Resultados/codigo_ejecucion.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Production script&lt;/a&gt;: executes all models and obtains the results.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conclusion&#34; style=&#39;text-align: left; color: #f76497;&#39;&gt;Conclusion&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Warehouse costs and stock-outs have been reduced by developing a scalable set of machine learning models that predict the demand in the next 8 days at store-product level.&lt;/p&gt;
&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
