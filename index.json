[{"authors":null,"categories":null,"content":" My name is Pedro Cortés. I love solving problems. More specifically, I love solving problems using code, maths and business knowledge. I have always been an extremely curious person. That’s why 8 years ago I decided to study industrial engineering as it would allow me to learn how to deal with problems in many different fields.\nMy academic life has been particularly concerned with numerical computer simulation and real-life experimentation, especially in the field of fluid mechanics. During my master’s degrees I had the opportunity to spend a semester in Sweden, where I had my first contact with algorithmic computing and artificial intelligence, which planted the seed of my interest in data science.\nAfter that, I started working as a facilities project engineer in the construction and retail sector. I realised that it was those projects with a high automation and data analysis component the ones I enjoyed the most. Therefore, I decided to water the data scientist seed inside me by updating my knowledge through a 6-month intensive training in artificial intelligence, machine learning, business analytics and data science with Python, SQL and Tableau.\nMy goal now is to start my career as a data scientist to help people and companies be more productive, make better decisions and create data-driven assets.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"My name is Pedro Cortés. I love solving problems. More specifically, I love solving problems using code, maths and business knowledge. I have always been an extremely curious person. That’s why 8 years ago I decided to study industrial engineering as it would allow me to learn how to deal with problems in many different fields.","tags":null,"title":"Pedro Cortés Macías","type":"authors"},{"authors":null,"categories":null,"content":"Table of contents Introduction Objectives Understanding the problem Project design Data quality Exploratory data analysis Feature transformation Lead segmentation model Lead scoring model Model deployment - Web app Launch Lead Score Analyzer Web App! Introduction The client is an online education company which sells an online course to industry professionals.\nThe company markets their course on different websites and search engines. Once professionals who are interested in the course land on the website, they might browse the course or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals.\nOnce these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not, with the inefficiency of this process impacting company’s benefits.\nNotes:\nThis article presents a technical explanation of the development process followed in the project. Source code can be found here. Feel free to test the developed web application here. Objectives Analysing historical leads information to propose potential actions that increase overall company turnover.\nCreating advanced analytical assets such as a predictive lead scoring and customer segmentation algorithms that helps sales team to identify both potential customers who are most likely to convert into paying customers and leads who are not economically profitable to manage.\nUnderstanding the problem When most companies start implementing inbound marketing, they’re primarily worried about getting enough new leads. However, once the marketing team defines and implements the right strategies and the systems the company has in place start working properly, getting enough leads is usually no longer a problem.\nInbound marketing stages. The challenge now is that in many cases the number of leads captured exceeds the capacity of the commercial channels, which generates problems such as:\nConflicts between marketing and sales departments: When there are so many leads coming in but not many sales being closed, these two departments may turn on one another. The marketing department does not understand how they are providing a large number of leads that Sales can not close. And the sales department believes that quality is more important than quantity, and they are not getting any good leads. Saturation of the commercial channel, given that each salesperson can manage a limited number of leads per day. Achieving fewer results than potentially possible, since there are a large number of leads that are not really interested in the company’s product and contact them takes time away from salespeople and prevents them from closing sales. For all the above reasons, it becomes necessary to prioritize in order to reach out to the “best” leads quickly, while saving the “less likely” leads for last or leaving these “less likely” leads to be handled by more automated channels such as email.\nNevertheless, businesses have struggled with prioritizing lead follow-up for decades. In many cases, salespeople are left to their own devices, using their best judgment to decide who gets contacted first. Marketers and salespeople use data such as demographic info (age, marital status, industry, role, …), to rank potential customers as to how likely they are to buy. The problem with this approach is that it has a certain component of subjectivity. Salespeople are forced to rely on “gut feelings” and factor in their own historical experience to make this decision. Neither of these proves to be consistently accurate causing quality leads to slip through the cracks as they chase prospects unlikely to buy.\nOn the other hand, to implement an effective inbound marketing strategy is vital to have knowledge about the type of leads interested in the company’s product to drive dynamic content and personalization tactics for timelier, relevant and more effective marketing communications. To this end, appropriate customer segmentation can help to cluster leads into groups sharing the same properties or behavioral characteristics instead of a ‘one-size-fits-all’ approach, which may prove to be in determine new market opportunities and improve brand strategy, marketing efficiency and customer retention between other benefits.\nProject design Methodology Traditionally there have been two main methodologies for advanced data modelling: CRISP-DM and SEMMA. Both methodologies structure the data mining project in phases that are interrelated, converting the process into iterative and interactive.\nTraditional advanced data modelling methodologies: CRISP-DM (left) and SEMMA (right). This project has been designed using a methodology halfway between the two presented above, as shown in the figure below.\nProject methodology. Levers There is usually a limitless number of …","date":1652832000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652832000,"objectID":"6d3c33bba838fc8a9f84b3b627f12be7","permalink":"https://pedrocorma.github.io/project/2leadscoring/","publishdate":"2022-05-18T00:00:00Z","relpermalink":"/project/2leadscoring/","section":"project","summary":"Increased lead-to-customer conversion rate from 38% to 50% of an online education company and reduced sales management time by 23% by developing lead segmentation and scoring models based on likelihood of purchase.","tags":["Machine Learning Automation Projects"],"title":"Lead Scoring and Segmentation","type":"project"},{"authors":null,"categories":null,"content":"Table of contents Introduction Objectives Understanding the problem Project design Data quality Exploratory data analysis Feature transformation Modelling Model deployment - Web app Launch Credit Risk Analyzer Web App! Introduction The client is an online platform which specialises in lending various types of loans to urban customers. Borrowers can easily access lower interest rate loans through a fast online interface.\nWhen the company receives a loan application, the company has to make a decision for loan approval based on the applicant’s profile. Like most other lending companies, lending loans to ‘risky’ applicants is the largest source of financial loss. The company aims to identify such ‘risky’ applicants and their associated loan’s expected loss in order to utilise this knowledge for managing its economic capital, portfolio and risk assessment.\nNotes:\nThis article presents an overview of the development process followed in the project. Source code can be found here. Feel free to test the developed credit risk analyzer web application here. Objectives Creating an advanced analytical asset based on machine learning predictive models to estimate the expected financial loss of each new customer-loan binomial.\nUnderstanding the problem Credit risk is associated with the possibility of a client failing to meet contractual obligations, such as mortgages, credit card debts, and other types of loans.\nMinimizing the risk of default is a major concern for financial institutions. For this reason, commercial and investment banks, venture capital funds, asset management companies and insurance firms are increasingly relying on technology to predict which clients are more prone to stop honoring their debts. Accounting for credit risk in the entire portfolio of instruments must consider the likelihood of future impairment and is commonly measured through expected loss and lifetime expected credit loss. To comply with IFRS 9 or CECL, risk managers need to calculate the expected credit loss on the portfolio of financial instruments over the lifetime of the portfolio.\nMachine Learning models have been helping these companies to improve the accuracy of their credit risk analysis, providing a scientific method to identify potential debtors in advance.\nProject design Methodology In order to estimate the expected loss (EL) associated to a certain loan application, three essential risk parameters have been considered: Probability of default (PD): is a measure of credit rating that is assigned internally to each customer with the aim of estimating the probability that a given customer will default. Loss given default (LGD): is another of the key metrics used in quantitative risk analysis. It is defined as the percentage risk of exposure that is not expected to be recovered in the event of default. Exposure at default (EAD): is defined as the percentage of outstanding debt at the time of default. Three predictive machine learning models will be developed to estimate these risk parameters, and their predictions will be combined to estimate the expected loss of each loan transaction as follows:\n$$ EL\\,(\\$) = PD (\\%) \\cdot P (\\$) \\cdot EAD (\\%) \\cdot LGD (\\%) $$ where P is the loan principal (the amount of money the borrower wishes to apply for).\nEL estimation by combining predictive machine learning models for PD, EAD and LGD. Note that the model for estimating the probability of default will be developed using a logistic regression algorithm. ‘Black box algorithms’ are not suitable in regulated financial services as their lack of interpretability and auditability could become a macro-level risk and in some cases, the law itself may dictate a degree of explainability. To overcome this problem, a highly explainable AI model, such as logistic regression, which provide details or reasons to make the functioning of AI clear or easy to understand, will be applied.\nFor the case of the exposure at default and loss given default models, different combinations of algorithms (Ridge, Lasso, LightGBM…) and hyperparameters have been tested to find those with the best performance. LightGBM algorithm was finally choosen in both cases.\nEntities and data The data under analysis is available at Lending Club website and contains information collected by the company regarding two main entities:\nBorrowers: There are features in the dataset capturing information on the applicant profile, i.e. applicant employment history, number of mortgages and credit lines, annual incomes and other personal information.\nLoans: The rest of the features provide information on the loan such as loan amount, loan interest rate, loan status (i.e. whether the loan is current or in default), loan tenor (either 36 or 60 months) between others.\nData quality In this stage of the project, general data quality correction processes have been applied, such as:\nFeature renaming Feature type correction Elimination of features with unique values Nulls imputation Outliers …","date":1651968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651968000,"objectID":"93b0f6f4c7ae91d913c2f006695fc74a","permalink":"https://pedrocorma.github.io/project/1riskscoring/","publishdate":"2022-05-08T00:00:00Z","relpermalink":"/project/1riskscoring/","section":"project","summary":"Automated the calculation of fees that make each new transaction/customer profitable by implementing a web application that predicts the expected loss based on probability of default, loss given default, and exposure at default models.","tags":["Machine Learning Automation Projects"],"title":"Credit Risk Scoring","type":"project"},{"authors":null,"categories":null,"content":"Table of contents Introduction Objectives Project design Methodology Taking requirements Data sources Required calculations Dashboard components Final Dashboard design Introduction The client is a hotel group specialized in the vacation market.\nObjectives Create a dashboard that company managers can use to start managing hotels in a more data driven way. Tableau Public software will be used for this purpose.\nProject design 1. Methodology In order to make the best use of the time and resources available, initial time has been invested in the development of the methodology to be followed during the project.\nDashboard design: Methodology. 2. Taking requirements In this first fundamental step the objective is to determine the requirements and scope of the project. To do this, meetings have been organised with the end users of the dashboard to understand what they expect from the use of the dashboard, what business problems they want to solve, what KPIs they want to measure, in what time frame they want to measure them, etc.\nThe following is a summary of the questions posed to the management team and their corresponding answers.\nWhat business objectives do you want to achieve by developing this dashboard? To have in a single report the main data necessary for hotel management. With current date and also with a view of developments in the last few months. Get the entire management to use the same data and share the same metrics. What data specifically would you like to visualize? Number of reservations Turnover Occupancy rate RevPar ADR Cancelation rate Through which dimensions would you like to see such metrics? All of the above metrics and that specific dates can be selected to consult the data. To know the evolution of the occupancy rate for the last 6 months and its seasonality. To know the bookings according to the customer’s country. What time frame do you need to visualize for the information? The longest possible history according to the information available in the data provided. At what time unit would you like to see that information? To know the value of all the mentioned metrics according to the dates to be consulted. Evolution of the occupancy rate over the last 6 months For the seasonality of the occupancy rate, display all available historical data. For bookings by country, use all available data. 3. Data sources The next stage of the project is to identify which data sources are available, where they are located, what information is available for each one, etc. In this case, the data provided by the company is contained in a single comma-separated values text file called ‘hotels.csv’.\nThis file directly contains the information of two of the metrics required by the client: the number of bookings and the ADR.\n3. Required calculations In this step the requirements demanded by the customer not directly covered by the information in the data sources are identified and it is determined how to calculate them through the existing information.\n4. Dashboard components Next, the most appropriate visualizations for each metric are selected and created.\nKPIs band Occupation rate evolution last 6 months Seasonality of occupation rate Client acquisition map 5. Final Dashboard design Finally, the dashboard design is addressed: its size and structure, its adaptation to different devices (pc, mobile, tablet...), dashboard level filters are set, etc. The developed dashboard is publicly available here. Dream Resort Hotels Dashboard - Desktop version. Dream Resort Hotels Dashboard - Tablet version. Dream Resort Hotels Dashboard - Phone version. ","date":1658966400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658966400,"objectID":"a494ed7b2664f4e89dea87d60dcc3957","permalink":"https://pedrocorma.github.io/project/6hotels/","publishdate":"2022-07-28T00:00:00Z","relpermalink":"/project/6hotels/","section":"project","summary":"Created a dashboard for a hotel group to visualize the main KPIs of the company (turnover, occupancy rate, cancellation rate...) in order to help the leadership team to manage the hotels in a more data driven way.","tags":["Dashboards"],"title":"Dreem Resort Hotels Dashboard","type":"project"},{"authors":null,"categories":null,"content":" Note: Current documentation available on the GitHub repository is in Spanish. It will soon be updated to English.\nTable of contents Introduction Objectives Understanding the problem Project design Data quality Exploratory data analysis Feature engineering and transformations Modelling Retraining and production scripts Conclusion Introduction The client is a large American retailer that desires to implement a sales prediction system based on artificial intelligence algorithms.\nNotes:\nThis article presents a technical explanation of the development process followed in the project. Source code can be found here. Objectives Developing a set of machine learning models on a three-year-history SQL database to predict sales for the next 8 days at the store-product level using massive modelling techniques.\nUnderstanding the problem One of the most frequent applications of data science is sales forecasting due to its direct impact on the balance sheet.\nSales forecasting is aimed at improving the following processes:\nSupplier relationship management: by having the prediction of customer demand in numbers, it’s possible to calculate how many products to order, making it easy for managers to decide whether the company needs new supply chains or to reduce the number of suppliers. Customer relationship management: customers planning to buy something expect the products they want to be available immediately. Demand forecasting allows to predict which categories of products need to be purchased in the next period from a specific store location. This improves customer satisfaction and commitment to the brand. Order fulfillment and logistics: sales forecasting features optimising supply chains. This means that at the time of order, the product will be more likely to be in stock, and unsold goods won’t occupy prime retail space. Marketing campaigns: forecasting is often used to adjust ads and marketing campaigns and can influence the number of sales. This is one of the use cases of machine learning in marketing. Sophisticated machine learning forecasting models can take marketing data into account as well. Manufacturing flow management: being part of the ERP, the time series-based demand forecasting predicts production needs based on how many goods will eventually be sold. To this end, traditional sales forecasting methods have been tried and tested for decades. With Artificial Intelligence development, they are now complemented by modern forecasting methods using Machine Learning. Machine learning techniques allows for predicting the amount of products/services to be purchased during a defined future period. In this case, a software system can learn from data for improved analysis. Compared to traditional demand forecasting methods, a machine learning approach allows to:\nAccelerate data processing speed Automate forecast updates based on the recent data Analyze more data Identify hidden patterns in data Increase adaptability to changes Massive and scalable machine learning modelling techniques will be used to approach the present project.\nProject design Project scope, entities and data This project has been developed based on the data contained in a three-year-history SQL database of a large American retailer. Due to the computational limitations of the equipment available for its development, the scope of the project has been limited to the sales forecasting of ten products belonging to a single category (food) in two different stores. However, the developed system is fully scalable to predict sales of more products, categories and stores by simply changing their respective parameters. Approach to the main forecasting-related problems 1. Hierarchical Forecasting More often than not, time series data follows a hierarchical aggregation structure. For example, in retail, sales for a Stock Keeping Unit (SKU) at a store can roll up to different category and subcategory hierarchies. In these cases, it must be assure that the sales estimates are in agreement when rolled up to a higher level. In these scenarios, Hierarchical Forecasting is used. It is the process of generating coherent forecasts (or reconciling incoherent forecasts) that allows individual time series to be forecasted individually while still preserving the relationships within the hierarchy.\nHierarchical structure in time series for store sales. Individual product sales are depicted in the lowest level (level 2) of the tree, followed by sales aggregated on the subcategory level (level 1), and finally all of the subcategory sales aggregated on the category level (level 0). 1.1. Problems:\nThere are different levels of hierarchies in the commercial catalog. It may be interesting to predict sales at different levels. Since the forecasts are probabilistic, predictions at different levels will not match exactly. 1.2. Solution:\nThe forecasts at all of the levels must be coherent. To achieve that, there are different reconciliation approaches to combining and breaking …","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"1afab139fafec755ff0c3072f3ce2d1e","permalink":"https://pedrocorma.github.io/project/0forecasting/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/project/0forecasting/","section":"project","summary":"Reduced warehouse costs and stock-outs by developing a scalable set of recursive machine learning models that predict the demand in the next 8 days at store-product level based on the data contained in the three-year-history company's SQL database.","tags":["Machine Learning Automation Projects"],"title":"Retail Sales Forecasting","type":"project"},{"authors":null,"categories":null,"content":" Note: Current documentation available on the GitHub repository is in Spanish. It will soon be updated to English.\nTable of contents Introduction Objectives Project design Data quality Exploratory data analysis Results communication Introduction The client is a Real Estate company that invests in large cities by buying properties to later rent them out as vacation apartments.\nThe managers have made the decision to invest in Madrid, and are interested in analysing the data that the sector leader AirBnb makes public to try to find the types of properties that have the greatest commercial potential for vacation rental.\nAs a main deliverable, the management expects to receive the typology (or typologies) of properties that the valuation team should look for among the existing opportunities in the city and the main neighborhoods or geographic areas to focus on.\nNotes:\nThis article presents a technical explanation of the development process followed in the project. Source code can be found here. Objectives Analysing available public data sources to find insights that help to understand the characteristics of the market in Madrid city and guide the valuation team’s research work, especially in terms of the main angles: rental prices, occupancy levels and purchase prices.\nProject design Levers There is usually a limitless number of things a company can consider trying in order to improve their business. However, the options should be narrowed down to include only the levers most relevant to the company and their situation and goals, which in this case are: Rental price: The more the company can charge per night the higher the profitability\nOccupancy level: In general, the more days per year a property can be rented, the higher its profitability.\nPurchase price: The cheaper the property can be acquired the greater the profitability\nKPIs In this case the Kpis are fairly straightforward: Occupancy will be measured as the number of days per year that the property can be rented. The rental price will be measured as the price per night in euros according to the Airbnb platform. The price of a property will be measured as the multiplication between the number of square meters and the average price per square meter in your area. A 25% discount will be applied on the official price to take into account the negotiating strength of the company’s experienced buying team. Entities and data Real data from the Airbnb and Idealista platforms have been used in this project. The entities relevant to the achievement of the project’s objectives and for which data are available are:\nProperties: location, price, room type, reviews, minimun/maximum nights, number of bedrooms, … Hosts: Name, id, url, location, verifications, … Districts: Neighborhood, district, … Data quality In this stage of the project, general data quality correction processes have been applied, such as:\nFeature renaming Feature type correction Elimination of features with unique values Nulls imputation Outliers management … The entire process can be consulted in detail here.\nExploratory data analysis The aim of this stage of the project is to discover trends, patterns, and to check assumptions with the help of statistical summary and graphical representations. Complete analysis can be found here.\nIn order to guide the process, a series of seed questions were posed to serve as a basis for developing and deepening the analysis of the different features.\nSeed questions Regarding rental price:\nQ1: What is the average price and price range, by districts and neighborhoods? Q2: What is the ranking of districts and neighborhoods by average rental price? Q3: What factors (other than location) determine the rental price? Q4: What is the relationship between the size of the property and the price at which it can be rented? Q5: How does competition (number of available properties per neighborhood) influence the rental price? Q6: How do prices vary by type of rental (whole apartment, private room, shared room)? Regarding occupancy level:\nQ7: What is the average occupancy level by district and by neighborhood? Q8: How likely is each occupancy level in each district? Q9: What is the ranking of districts and neighborhoods by occupancy level? Q10: What factors (other than location) determine occupancy level? Q11: What is the relationship between property size and occupancy level? Q12: How does competition (number of properties available per district) influence occupancy level? Regarding purchase price:\nQ13: What is the ranking of price per m2 by district? Q14: What is the ranking of property price (m2 * average size) by district? Q15: What is the relationship between property price and rental price by district? Q16: What is the relationship between property price and occupancy by district? Insights Once the exploratory data analysis has been carried out, the following executive conclusions have been obtained: 10 neighborhoods have been located in which to focus the search for …","date":1645142400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645142400,"objectID":"b421c4e143055c71af65de69a9231f1b","permalink":"https://pedrocorma.github.io/project/5realstate/","publishdate":"2022-02-18T00:00:00Z","relpermalink":"/project/5realstate/","section":"project","summary":"Analysed available public AirBnb data sources to find insights that help to understand the characteristics of the vacation rental market in Madrid city and guide the valuation team’s research work, especially in terms of rental prices, occupancy levels and purchase prices.","tags":["Discovery Projects"],"title":"AirBnb Vacation Rental Market Analysis","type":"project"},{"authors":null,"categories":null,"content":" Note: Current documentation available on the GitHub repository is in Spanish. It will soon be updated to English.\nTable of contents Introduction Objectives Project design Data quality Exploratory data analysis Results Introduction The client is an ecommerce in the cosmetics sector that seeks to break the trend of null evolution obtained during the last months.\nNotes:\nThis article presents a technical explanation of the development process followed in the project. Source code can be found here. Objectives Analysing the ecommerce transactional data to find potential conversion rate optimisation (CRO) actions that increase visits, conversions and the average ticket, and therefore increase overall ecommerce revenue.\nCreating advanced analytical assets such as RFM segmentation and a recommendation system to drive goal achievement.\nProject design Understanding the business model First step in selling products online is when a user arrives at the ecommerce website. Normally it will come from:\nPaid campaigns: paid ads such as Facebook Ads or Google Ads. Organic content: blog, rrss, … Direct traffic: the client knows the url and enter it directly in the browser. That traffic is called visits, and the pages that potential customers view are called page views.\nThe user browses the website and when he/she likes a product he/she puts it in the cart. Eventually the potential client can remove products from the cart, leave the website without buying anything, or finally place the order.\nA common process in this type of business is cross-selling, in which the user is recommended other products that might also be of interest to him/her. Even when the user has left the website, it is possible to contact him again through retargeting or email marketing. This whole process is called funnel or customer journey.\nEcommerce Customer journey. In the online environment virtually everything can be registered. The customer’s registration can be logged, his/her sequence of actions in the same browsing session can be tracked, etc.\nRegarding the metrics used in this type of business, one of the most important is the ratio of purchases to visits, also known as conversion ratio. There are also other key metrics that are necessary to know in order to manage an ecommerce properly:\nCPA: Cost Per Acquisition AOV: Average Order Value Frequency of Purchase LTV: Life Time Value Churn: A measure of how many customers stop using a product. It is more applicable to recurring payment ecommerces (SaaS, etc…). Levers There are three main ways to grow this type of business: obtaining new customers (which means getting more visits and higher conversion rates), increasing the purchase frequency of existing customers, and increasing the average ticket value, which means getting more or higher value products purchased in the same shopping session. In order to achieve these 3 effects, this project will work on the following operational levers: Customer journey: How every step of the journey can be optimised? Customers: How can the available customer information be used to optimise the company’s commercial campaigns? Products: How can the product catalog be optimised? How can the company identify in a personalised way the best products to show to each customer? KPIs Visits Conversion rate Purchase frequency AOV: Average Order Value Shopping Cart Abandonment Rate LTV: Life Time Value Entities and data The existing entities in the granularity of ecommerce available data are: Users: id, session… Clients: number of products purchased, average order cost, frequency of purchase… Sessions: session ID. Events: visits, products added to cart, sales, event time… Products: price, number of units sold, brand… Data quality In this stage of the project, general data quality correction processes have been applied, such as:\nFeature renaming Feature type correction Elimination of features with unique values Nulls imputation Outliers management … The entire process can be consulted in detail here.\nExploratory data analysis The aim of this stage of the project is to discover trends, patterns, and to check assumptions with the help of statistical summary and graphical representations. Complete analysis can be found here.\nIn order to guide the process, a series of seed questions were posed to serve as a basis for developing and deepening the analysis of the different features.\nSeed questions Regarding customer journey:\nQ1: What does a typical shopping process look like? Q2: How many products are viewed, added to cart, abandoned and purchased on average per session? Q3: How have these metrics been trending in recent months? Regarding clients:\nQ4: How many products does each customer buy? Q5: How much does each customer spend on average? Q6: Are there ‘good customers’ that need to be identified and treated differently? Q7: Do customers repeat purchases in the following months? Q8: What is the average LTV of a customer? Q9: Can campaigns can be tailored to customer’s value? …","date":1646784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646784000,"objectID":"baad4385c11425e962cfd6466a363a2f","permalink":"https://pedrocorma.github.io/project/3ecommerce/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/project/3ecommerce/","section":"project","summary":"Developed a conversion rate optimisation (CRO) plan of 10 specific initiatives organized into 5 major business levers (number of views, conversion rate, cross-selling, purchase frequency and customer loyalty) to break the flat evolution of the company's KPIs over the last few months and achieve an overall increase in ecommerce revenues.","tags":["Discovery Projects"],"title":"Ecommerce Optimisation","type":"project"},{"authors":null,"categories":null,"content":" Note: Current documentation available on the GitHub repository is in Spanish. It will soon be updated to English.\nTable of contents Introduction Objectives Project design Data quality Exploratory data analysis Results Introduction The client is a renewable energy generation company that has detected anomalous behaviors in two of its photovoltaic solar plants. Their maintenance subcontractor is unable to identify the reasons for the inefficiencies and they wish to perform an in-depth analysis of the plants’ sensor and meter data before mobilizing their engineering team.\nNotes:\nThis article presents a technical explanation of the development process followed in the project. Source code can be found here. Objectives Analyse the available data to try to figure out where the problems may be and whether or not it is necessary to send a engineering team to the plants.\nProject design Understanding the operation of a photovoltaic solar plant In order to identify the business levers on which to focus the analysis, it is necessary to understand the basic operation of a photovoltaic solar plant.\nPhotovoltaic solar plants directly convert solar energy into electricity. They work on the principle of the photovoltaic effect. When certain materials are exposed to light, they absorb photons and release free electrons. This phenomenon is called as the photoelectric effect, wich is a method of producing direct current electricity based on the principle of the photoelectric effect.\nPhotovoltaic solar plant scheme. Based on the principle of photovoltaic effect, solar cells or photovoltaic cells are made. They convert sunlight into direct current (DC) electricity. But, a single photovoltaic cell does not produce enough amount of electricity. Therefore, a number of photovoltaic cells are mounted on a supporting frame and are electrically connected to each other to form a photovoltaic module or solar panel. According to the requirement of power, multiple photovoltaic modules are electrically connected together to form a PV array and to achieve more power.\nThe generated DC electricity is then converted to alternating current (AC) which is cheaper and easier to transport and is the type of electrical current used when you plug appliances into normal wall sockets. This transformation process is accomplished by means of a device called inverter.\nOn the other hand, within the plant ecosystem there are also power meters (for measuring both DC and AC current) and sensors of different types (temperature, irradiation, …).\nFinally, the AC current is transported to the different consumers through the distribution network.\nLevers Once the fundamentals of the operation of a PV solar plant have been analised, the levers that influence the business objective (in this case generating AC) have been identified:\nSolar irradiation: the higher the irradiation, the higher the DC generated. However, it is not monotonic; above certain values, higher temperatures decrease the generation capacity. Panels conditions: panels have to be clean and in proper working conditions in order to generate as much DC as possible. Inverters efficiency: there is always a loss in the transformation from DC to AC, but it should be the minimum possible. Inverters have also to be in good conditions. Meters and sensors: if they break down and/or do not measure properly, traceability and the possibility of detecting faults is lost. KPIs Solar irradiation: measures incoming solar energy. Ambient and panel temperature: measured by the plant’s sensors in degrees Celsius. DC power: measures the generated kW of direct current. AC power: measures the generated kW of alternating current. Inverter efficiency: measures the transformation capacity from DC to AC. It is calculated as $\\dfrac{AC}{DC} \\cdot 100$. Entities and data To determine the entities it is necessary to know what a solar plant is composed of:\nThe minimum unit is the cell, where the generation of energy by reaction with the sun’s photons takes place. Cells are encapsulated in ‘rectangles’ called modules. Several modules form a panel. Panels are arranged in rows called arrays. An inverter receives direct current from several arrays. A plant can have several inverters. There are also meters and sensors. In the present project the existing entities in the granularity of the data provided by the company are:\nData in 15-minute windows over a period of 34 days. Solar photovoltaic plants: two of them. Inverters: several per plant. A single irradiance sensor per plant. A single ambient temperature sensor per plant. A single module temperature sensor per plant. This information will condition the analyses that can be carried out. For example, it will be possible to determine if an inverter in a plant is underperforming, but it will not be possible to determine which array, panel or module may be causing these inefficiencies.\nData quality In this stage of the project, general data quality correction processes have been applied, …","date":1645920000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645920000,"objectID":"7d54f8133bc953fe2a30fdb1d9f33b7b","permalink":"https://pedrocorma.github.io/project/4solarplant/","publishdate":"2022-02-27T00:00:00Z","relpermalink":"/project/4solarplant/","section":"project","summary":"Found the source of existing efficiency problems in two photovoltaic solar plants belonging to a renewable energy generation company by analysing data from their sensors and meters (irradiation, temperature, inverter efficiency, power...) in order to guide and optimize the efforts of the maintenance engineering team.","tags":["Discovery Projects"],"title":"Inefficiency Detection in Photovoltaic Solar Plants","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://pedrocorma.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]